{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import getpass\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import chain\n",
    "import datetime\n",
    "from fractions import Fraction\n",
    "from collections import Counter\n",
    "from pymongo import UpdateOne\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "import dask.dataframe as dd\n",
    "import html\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_book = \"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\books_data.csv\"\n",
    "# filepath = \"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\Books_rating.csv\"\n",
    "\n",
    "df_book = pd.read_csv(filepath_book)\n",
    "# df_review = pd.read_csv(filepath)\n",
    "\n",
    "password = 'Ngthiennhan2002.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_index = 0\n",
    "# chunk_size = 375000\n",
    "\n",
    "# for i in range(1, 9):\n",
    "#     end_index = start_index + chunk_size\n",
    "#     df_chunk = df_review.iloc[start_index:end_index]\n",
    "#     file_name = f\"D:\\\\University\\\\Môn học\\\\Ứng dụng Big Data\\\\Project\\\\Datasets\\\\Amazon Books Reviews\\\\Books_rating_{i}_375k.csv\"\n",
    "#     df_chunk.to_csv(file_name, index=False)\n",
    "#     start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cryptography\\x509\\base.py:594: CryptographyDeprecationWarning: Parsed a negative serial number, which is disallowed by RFC 5280.\n",
      "  return rust_x509.load_der_x509_certificate(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375000\n",
      "304791\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "212404\n"
     ]
    }
   ],
   "source": [
    "API_1 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.mhlvibl.mongodb.net/'\n",
    "# API_1 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.yl3o8ez.mongodb.net/'\n",
    "API_2 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.bzvhw41.mongodb.net/'\n",
    "API_3 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.jrdv2e2.mongodb.net/'\n",
    "API_4 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.tq84xea.mongodb.net/'\n",
    "API_5 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.i2p6hb8.mongodb.net/'\n",
    "API_6 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.ppuo86b.mongodb.net/'\n",
    "API_7 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.xjtimov.mongodb.net/'\n",
    "API_8 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.5ihto1h.mongodb.net/'\n",
    "API_items = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.2qutpiu.mongodb.net/'\n",
    "\n",
    "try:\n",
    "    # Create two MongoDB clients using MongoClient with two APIs\n",
    "    client_1 = MongoClient(API_1)\n",
    "    client_2 = MongoClient(API_2)\n",
    "    client_3 = MongoClient(API_3)\n",
    "    client_4 = MongoClient(API_4)\n",
    "    client_5 = MongoClient(API_5)\n",
    "    client_6 = MongoClient(API_6)\n",
    "    client_7 = MongoClient(API_7)\n",
    "    client_8 = MongoClient(API_8)\n",
    "    client_items = MongoClient(API_items)\n",
    "    \n",
    "    # Variables to save names of databases and collections\n",
    "    database_name = 'db'\n",
    "    review_collection_name = 'Reviews'\n",
    "    book_collection_name = 'Books'\n",
    "    author_collection_name = 'Authors'\n",
    "    categories_collection_name = 'Categories'\n",
    "    temp_collection_name = 'Temp'\n",
    "    user_collection_name = 'Users'\n",
    "\n",
    "    db_1 = client_1[database_name]\n",
    "    review_collection_1 = db_1[review_collection_name]\n",
    "\n",
    "    db_2 = client_2[database_name]\n",
    "    review_collection_2 = db_2[review_collection_name]\n",
    "    \n",
    "    db_3 = client_3[database_name]\n",
    "    review_collection_3 = db_3[review_collection_name]\n",
    "\n",
    "    db_4 = client_4[database_name]\n",
    "    review_collection_4 = db_4[review_collection_name]\n",
    "    \n",
    "    db_5 = client_5[database_name]\n",
    "    review_collection_5 = db_5[review_collection_name]\n",
    "\n",
    "    db_6 = client_6[database_name]\n",
    "    review_collection_6 = db_6[review_collection_name]\n",
    "    \n",
    "    db_7 = client_7[database_name]\n",
    "    review_collection_7 = db_7[review_collection_name]\n",
    "\n",
    "    db_8 = client_8[database_name]\n",
    "    review_collection_8 = db_8[review_collection_name]\n",
    "    \n",
    "    db_items = client_items[database_name]\n",
    "    book_collection = db_items[book_collection_name]\n",
    "    author_collection = db_items[author_collection_name]\n",
    "    categories_collection = db_items[categories_collection_name]\n",
    "    temp_collection = db_items[temp_collection_name]\n",
    "    users_collection = db_items[user_collection_name]\n",
    "except:\n",
    "    print(\"Incorrect password or cannot connect to MongoDB at this time\")\n",
    "    \n",
    "# Test if the documents are read successfully\n",
    "print(review_collection_1.count_documents({}))\n",
    "print(review_collection_2.count_documents({}))\n",
    "print(review_collection_3.count_documents({}))\n",
    "print(review_collection_4.count_documents({}))\n",
    "print(review_collection_5.count_documents({}))\n",
    "print(review_collection_6.count_documents({}))\n",
    "print(review_collection_7.count_documents({}))\n",
    "print(review_collection_8.count_documents({}))\n",
    "print(book_collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lengths of collections\n",
    "total_book = book_collection.count_documents({})\n",
    "total_review = 375000\n",
    "\n",
    "# Convert book collection into pandas DataFrame (with progress bar)\n",
    "# with tqdm(total=total_book, desc='Converting book collection') as pbar:\n",
    "#     df_book = pd.DataFrame(list(book_collection.find()))\n",
    "#     pbar.update(len(df_book))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 1') as pbar:\n",
    "#     df_review_1 = pd.DataFrame(list(review_collection_1.find()))\n",
    "#     pbar.update(len(df_review_1))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 2') as pbar:\n",
    "#     df_review_2 = pd.DataFrame(list(review_collection_2.find()))\n",
    "#     pbar.update(len(df_review_2))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 3') as pbar:\n",
    "#     df_review_3 = pd.DataFrame(list(review_collection_3.find()))\n",
    "#     pbar.update(len(df_review_3))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 4') as pbar:\n",
    "#     df_review_4 = pd.DataFrame(list(review_collection_4.find()))\n",
    "#     pbar.update(len(df_review_4))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 5') as pbar:\n",
    "#     df_review_5 = pd.DataFrame(list(review_collection_5.find()))\n",
    "#     pbar.update(len(df_review_5))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 6') as pbar:\n",
    "#     df_review_6 = pd.DataFrame(list(review_collection_6.find()))\n",
    "#     pbar.update(len(df_review_6))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 7') as pbar:\n",
    "#     df_review_7 = pd.DataFrame(list(review_collection_7.find()))\n",
    "#     pbar.update(len(df_review_7))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 8') as pbar:\n",
    "#     df_review_8 = pd.DataFrame(list(review_collection_8.find()))\n",
    "#     pbar.update(len(df_review_8))\n",
    "#     pbar.close()\n",
    "\n",
    "# print(\"--- Finished converting to DataFrame ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two df_review_1 and df_review_2\n",
    "# with tqdm(total=total_review, desc='Combining 2 review collections') as pbar:\n",
    "#     df_review = pd.concat([df_review_1, df_review_2, df_review_3, df_review_4, df_review_5, df_review_6, df_review_7, df_review_8])\n",
    "#     pbar.update(len(df_review))\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Title\\\n",
    "Missing values: 1 book thiếu Title -> điền thủ công bằng cách vào Link\\\n",
    "Noises: Chuyển chuỗi về kí tự thường và loại bỏ các kí tự đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count = 0\n",
    "\n",
    "# # for temp in temp_collection.find():\n",
    "# #     if temp['field'] == 'book_title':\n",
    "# #         temp_id = temp['_id']\n",
    "# #         count = int(temp['value'])\n",
    "# #         break\n",
    "\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Title', position=0)\n",
    "\n",
    "# title_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     title = book['Title']\n",
    "#     if type(title) is not str: \n",
    "#         title = str(title)\n",
    "#     title = title.lower()\n",
    "#     title = remove_special_characters(title)\n",
    "#     title = re.sub(r'\\s+', ' ', title)\n",
    "#     title_list.append(({'_id': book['_id']}, {'$set': {'Title': title}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for title in title_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             title[0],\n",
    "#             title[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Description\n",
    "- Missing values: Bỏ qua\n",
    "- Noises: Chuyển chuỗi về chữ thường và loại bỏ các kí tự đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Description', position=0)\n",
    "\n",
    "# description_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     if 'description' in book:\n",
    "#         description = str(book['description'])\n",
    "#         description = description.lower()\n",
    "#         description = remove_special_characters(description)\n",
    "#         description = re.sub(r'\\s+', ' ', description)\n",
    "#         description_list.append(({'_id': book['_id']}, {'$set': {'description': description}}))\n",
    "#     # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for description in description_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             description[0],\n",
    "#             description[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Authors và Categories\n",
    "- Missing values: Bỏ qua\n",
    "- Tạo danh sách chuỗi các tác giả\n",
    "- Noises: Chuyển chuỗi về chữ thường, loại bỏ các kí tự đặc biệt và xử lý các biến thể tên tác giả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = []\n",
    "# categories = []\n",
    "\n",
    "# for author in author_collection.find():\n",
    "#     author_id = author['_id']\n",
    "#     authors.append(({'_id': author_id}, {'$set': {'Value': author['Value']}}))\n",
    "    \n",
    "# for category in categories_collection.find():\n",
    "#     category_id = category['_id']\n",
    "#     categories.append(({'_id': category_id}, {'$set': {'Value': category['Value']}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=author_collection.count_documents({}), desc='Preprocessing Authors\\'s collection', position=0)\n",
    "\n",
    "# author_list = []\n",
    "\n",
    "# for author in author_collection.find():\n",
    "#     author_name = author['Value']\n",
    "#     if author_name:\n",
    "#         author_name = author_name.lower()\n",
    "#         author_name = remove_special_characters(author_name)\n",
    "#         author_name = re.sub(r'\\s+', ' ', author_name)\n",
    "#         author_list.append(({'_id': author['_id']}, {'$set': {'Value': author_name}}))\n",
    "#     # author_list.append({'_id': temp_id}, {'$set': {'Value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for author in author_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             {'_id': author[0]['_id']},\n",
    "#             {'$set': {'Value': author[1]['$set']['Value']}},\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = author_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=categories_collection.count_documents({}), desc='Preprocessing Categories\\'s collection', position=0)\n",
    "\n",
    "# category_list = []\n",
    "\n",
    "# for category in categories_collection.find():\n",
    "#     category_name = category['Value']\n",
    "#     category_name = category_name.lower()\n",
    "#     category_name = remove_special_characters(category_name)\n",
    "#     category_name = re.sub(r'\\s+', ' ', category_name)\n",
    "#     category_list.append(({'_id': category['_id']}, {'$set': {'Value': category_name}}))\n",
    "#     # temp_collection.update_one({'_id': temp_id}, {'$set': {'Value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for category in category_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             category[0],\n",
    "#             category[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = categories_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Publisher\n",
    "- Missing values: Bỏ qua\n",
    "- Noises: Loại bỏ kí tự đặc biệt, chuyển từ chữ hoa sang chữ thường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Publisher', position=0)\n",
    "\n",
    "# publisher_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     if 'publisher' in book:\n",
    "#         publisher = book['publisher']\n",
    "#         # print(publisher)\n",
    "#         if type(publisher) is int:\n",
    "#             publisher = str(publisher)\n",
    "#         publisher = publisher.lower()\n",
    "#         publisher = remove_special_characters(publisher)\n",
    "#         publisher = re.sub(r'\\s+', ' ', publisher)\n",
    "#         publisher_list.append(({'_id': book['_id']}, {'$set': {'publisher': publisher}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for publisher in publisher_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             publisher[0],\n",
    "#             publisher[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = book_collection.bulk_write(update_requests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường publishedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# def convert_to_datetime(date_str):\n",
    "#     formats = ['%Y', '%Y-%m', '%m-%Y', '%Y-%m-%d', '%m-%d-%Y', '%Y/%m/%d', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M', '%m/%d/%Y', '%d/%m/%Y']\n",
    "#     if isinstance(date_str, datetime):\n",
    "#         return date_str\n",
    "    \n",
    "#     if isinstance(date_str, int):\n",
    "#         date_str = str(date_str)\n",
    "        \n",
    "#     for fmt in formats:\n",
    "#         try:\n",
    "#             date_obj = datetime.strptime(date_str, fmt)\n",
    "#             if fmt.count('%') == 1:\n",
    "#                 return date_obj.strftime('%Y')\n",
    "#             elif fmt.count('%') == 2:\n",
    "#                 return date_obj.strftime('%Y-%m')\n",
    "#             else:\n",
    "#                 return date_obj.strftime('%Y-%m-%d')\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "#     return None\n",
    "    \n",
    "# count = 0\n",
    "\n",
    "# a = []\n",
    "\n",
    "# progress_bar = tqdm(total=total_book-count, desc='Preprocessing book collection\\'s Published Date', position=0)\n",
    "\n",
    "# for book in book_collection.find().skip(count):\n",
    "#     if 'publishedDate' in book:\n",
    "#         publishedDate = book['publishedDate']\n",
    "#         if publishedDate:\n",
    "#             try:\n",
    "#                 publishedDate = convert_to_datetime(publishedDate)\n",
    "#                 # book_collection.update_one({'_id': book['_id']}, {'$set': {'publishedDate': publishedDate}})\n",
    "#                 a.append(({'_id': book['_id']}, {'$set': {'publishedDate': publishedDate}}))\n",
    "#             except ValueError:\n",
    "#                 print(\"Error\")\n",
    "#         count += 1\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# published_date_list = a\n",
    "\n",
    "# for published_date in published_date_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             published_date[0],\n",
    "#             published_date[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Title (của Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_collections = [review_collection_1,\n",
    "                     review_collection_2,\n",
    "                     review_collection_3,\n",
    "                     review_collection_4,\n",
    "                     review_collection_5,\n",
    "                     review_collection_6,\n",
    "                     review_collection_7,\n",
    "                     review_collection_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, review_collection in enumerate(review_collections):\n",
    "    \n",
    "#     print(f'--- Preprocessing title of review collection {index + 1} ---')\n",
    "#     title_list = []\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review collection\\'s Title', position=0)\n",
    "    \n",
    "#     for index_2, review in enumerate(review_collection.find()):\n",
    "#         if 'Title' in review:\n",
    "#             title = review['Title']\n",
    "#             if type(title) is not str: \n",
    "#                 title = str(title)\n",
    "#             # title = title.lower()\n",
    "#             # title = remove_special_characters(title)\n",
    "#             title = re.sub(r'\\s+', ' ', title)\n",
    "#             title_list.append(({'_id': review['_id']}, {'$set': {'Title': title}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()\n",
    "    \n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     update_requests = []\n",
    "    \n",
    "#     for title in title_list:\n",
    "#         update_requests.append(\n",
    "#             UpdateOne(\n",
    "#                 title[0],\n",
    "#                 title[1],\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     print(update_requests[:5])\n",
    "\n",
    "#     # # Thực hiện bulk write\n",
    "#     # result = review_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường review/count và trường review/average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sử dụng defaultdict để tổ chức dữ liệu theo 'Title'\n",
    "# title_data = defaultdict(list)\n",
    "\n",
    "# # Thu thập dữ liệu từ các collection\n",
    "# for review_collection in review_collections:\n",
    "#     documents = review_collection.find({'review/score': {'$exists': True}})\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review scores', position=0)\n",
    "    \n",
    "#     for document in documents:\n",
    "#         if 'Title' in document:\n",
    "#             title = document['Title']\n",
    "#             title_data[title].append(document['review/score'])\n",
    "#             progress_bar.update(1)\n",
    "\n",
    "#     progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=len(title_data), desc='Calculating review scores', position=0)\n",
    "\n",
    "# new_book_scores_list = list()\n",
    "\n",
    "# for title, scores_list in title_data.items():\n",
    "#     count_reviews = len(scores_list)\n",
    "#     if scores_list:\n",
    "#         mean_score = statistics.mean(scores_list)\n",
    "#         median_score = statistics.median(scores_list)\n",
    "#         mode_score = statistics.mode(scores_list)\n",
    "#     else:\n",
    "#         mean_score, median_score, mode_score = 0, 0, 0\n",
    "        \n",
    "#     book_score_info = ({'Title': title}, {'$set': {'review/mean_score': mean_score, \n",
    "#                                                        'review/median_score': median_score,\n",
    "#                                                        'review/mode_score': mode_score,\n",
    "#                                                        'review/count': count_reviews}})\n",
    "#     new_book_scores_list.append((book_score_info))\n",
    "\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for book in new_book_scores_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             book[0],\n",
    "#             book[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews\n",
    "\n",
    "- Noises: Đã thực hiện ở trên\n",
    "- Missing values: Fill theo 'Id' nếu có 'Id' nào chứa Title đó. nếu không có để trống"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sử dụng defaultdict để tổ chức dữ liệu theo 'Title'\n",
    "# id_data = defaultdict(list)\n",
    "\n",
    "# progress_bar = tqdm(total=len(review_collections), desc='Preprocessing miss Review Titles', position=0)\n",
    "\n",
    "# # Thu thập dữ liệu từ các collection\n",
    "# for review_collection in review_collections:\n",
    "#     documents = review_collection.find({})\n",
    "    \n",
    "#     null_titles = review_collection.find({'Title': {'$exists': False}})\n",
    "    \n",
    "#     for document in null_titles:\n",
    "#         existed_titles_by_id_1 = review_collection_1.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_2 = review_collection_2.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_3 = review_collection_3.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_4 = review_collection_4.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_5 = review_collection_5.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_6 = review_collection_6.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_7 = review_collection_7.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_8 = review_collection_8.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "        \n",
    "#         for existed_review in existed_titles_by_id_1:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_2:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_3:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_4:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_5:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_6:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_7:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_8:\n",
    "#             print(existed_review)\n",
    "    \n",
    "#     progress_bar.update(1)\n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     # update_requests = []\n",
    "\n",
    "#     # for published_date in published_date_list:\n",
    "#     #     update_requests.append(\n",
    "#     #         UpdateOne(\n",
    "#     #             published_date[0],\n",
    "#     #             published_date[1],\n",
    "#     #         )\n",
    "#     #     )\n",
    "        \n",
    "#     # print(update_requests[:5])\n",
    "\n",
    "#     # # Thực hiện bulk write\n",
    "#     # result = review_collection.bulk_write(update_requests)\n",
    "\n",
    "# progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết luận: Không có Title bị khuyết nào có thể được điền thông qua chỉ số Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường review/helpfulness\n",
    "- Chuyển từ dạng a/b thành dạng float\n",
    "- Thay missing values = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, review_collection in enumerate(review_collections):\n",
    "    \n",
    "#     print(f'--- Preprocessing title of review collection {index + 1} ---')\n",
    "#     helpfulness_list = []\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review collection\\'s Helpfulness', position=0)\n",
    "    \n",
    "#     for index_2, review in enumerate(review_collection.find()):\n",
    "#         if 'review/helpfulness' in review:\n",
    "#             helpfulness_string = review['review/helpfulness']\n",
    "#             helpfulness_string = helpfulness_string.split('/')\n",
    "#             numerator = int(helpfulness_string[0])\n",
    "#             denominator = int(helpfulness_string[1])\n",
    "            \n",
    "#             # Nếu mẫu bằng 0\n",
    "#             if denominator == 0.0 or numerator == 0.0:\n",
    "#                 helpfulness_value = 0.0\n",
    "#             else:\n",
    "#                 helpfulness_value = numerator / denominator\n",
    "#                 if helpfulness_value > 1.0: # Xử lý các outlier lớn hơn 1\n",
    "#                     helpfulness_value = 1.0\n",
    "#         else:\n",
    "#             helpfulness_value = 0.0\n",
    "\n",
    "#         helpfulness_list.append(({'_id': review['_id']}, {'$set': {'review/helpfulness': helpfulness_value}}))\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()\n",
    "    \n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     update_requests = []\n",
    "    \n",
    "#     for helpfulness in helpfulness_list:\n",
    "#         update_requests.append(\n",
    "#             UpdateOne(\n",
    "#                 helpfulness[0],\n",
    "#                 helpfulness[1],\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     print(update_requests[:5])\n",
    "\n",
    "#     # Thực hiện bulk write\n",
    "#     result = review_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng Model:\n",
    "- SENTIMENT ANALYSIS đối với review/summary và review/text (RoBERTa-base model): biến chuỗi text thành dạng về một số (1: POS, 0: NEU, -1: NEG)\n",
    "- Tìm keyword của phần Description (BERT) (VD: Từ đoạn text \"Harry Potter this year fights Lord Voldemort and his Death Eaters\" -> [\"Harry Potter\", \"Lord Voldemort\", \"Death Eaters\"] -> \"Harry Potter Lord Voldemort Death Eaters\") -> Tính TF-IDF của phần Description giữa review input và toàn bộ review trong database -> Cho ra similarity với Description đầu vào\n",
    "- Tính TF-IDF các mục Authors, Title, Categories -> Cho ra similarity với Authors, Title, Categories đầu vào \n",
    "- Tính kNN (cosine similarity) giữa vector review vừa được thêm vào với tất cả review còn lại rồi trả về top 10 quyển có chỉ số sim cao nhất\n",
    "\n",
    "Cuối cùng là display nó lên cho người dùng xem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = []\n",
    "\n",
    "# for index, review_collection in enumerate(review_collections):\n",
    "#     print(f' --- Start converting review collection {index + 1} ---')\n",
    "#     df_review = pd.DataFrame(list(review_collection.find()))\n",
    "    \n",
    "#     dd_review = dd.from_pandas(df_review, npartitions=4)\n",
    "#     print(f' --- Done converting review collection {index + 1} ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = df.concat(df_reviews)\n",
    "# df_book = book_collection.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book_selected = df_book.loc[:, ['Title', 'authors', 'categories', 'description', 'review/count', 'review/mean_score', 'review/median_score', 'review/mode_score']]\n",
    "# df_reviews_selected = df_reviews.loc[:, ['Title', 'User_id', 'review/score', 'review/summary', 'review/text']]\n",
    "# merged_df = pd.merge(df_reviews_selected, df_book_selected, on='Title', how='inner')\n",
    "\n",
    "# merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv('D:\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file all.csv lên lại để đưa vào mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    # text = \"This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the place to find it -- there's only about 2 pages with text and everything else is photos.Bottom line: if you only want one book, the Six Foot One ... is probably a better choice, however, if you like Julie like I like Julie, you won't go wrong on this one either.\"\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model_sentiment(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "\n",
    "    highest_score = max(scores)\n",
    "    label = config.id2label[ranking[0]]\n",
    "    \n",
    "    if label == 'positive':\n",
    "        sentiment = 1\n",
    "    elif label == 'neutral':\n",
    "        sentiment = 0\n",
    "    elif label == 'negative':\n",
    "        sentiment = -1\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffile = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "\n",
    "# ffile['sentiment'] = ''\n",
    "\n",
    "# ffile.to_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "#                     #  chunksize=CHUNK_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# count = int(3500000 / CHUNK_SIZE)\n",
    "\n",
    "# sentiment_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=count, desc=f'Processing Sentiment Analysis...', position=0)\n",
    "\n",
    "# with open('sentiment.json', 'w') as json_file:\n",
    "    \n",
    "#     for chunk in df_all:\n",
    "        \n",
    "#         for index, row in chunk.iterrows():\n",
    "            \n",
    "#             review_text = html.unescape(row['review/text'])\n",
    "            \n",
    "#             title = row['Title']\n",
    "#             if title == '' or title == None:\n",
    "#                 continue\n",
    "            \n",
    "#             user_id = str(row['User_id'])\n",
    "#             if user_id == '' or user_id == None:\n",
    "#                 user_id = ''\n",
    "            \n",
    "#             if review_text == '' or review_text == None:\n",
    "#                 sentiment = 0\n",
    "#                 break\n",
    "\n",
    "#             sentiment_score, label = sentiment_analysis(review_text[:max_length])\n",
    "\n",
    "#             if label == 'positive':\n",
    "#                 sentiment = 1\n",
    "#             elif label == 'neutral':\n",
    "#                 sentiment = 0\n",
    "#             elif label == 'negative':\n",
    "#                 sentiment = -1\n",
    "                \n",
    "#             sentiment_json = {'Title': title, 'User_id': user_id, 'sentiment': sentiment}\n",
    "#             json.dump(sentiment_json, json_file)\n",
    "#             json_file.write(',\\n')\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 512\n",
    "\n",
    "# count = 0\n",
    "# sentiment_list = []\n",
    "\n",
    "# for review_collection in review_collections:\n",
    "\n",
    "#     reviews = review_collection.find()\n",
    "#     existed_num = review_collection.count_documents({'sentiment': {'$exists': True}})\n",
    "#     progress_bar = tqdm(total= (total_review - existed_num), desc=f'Processing Sentiment Analysis...', position=0)\n",
    "    \n",
    "#     for review in reviews.skip(existed_num):\n",
    "        \n",
    "#         review_text = html.unescape(str(review['review/text']))\n",
    "        \n",
    "#         if review_text == '' or review_text is None:\n",
    "#             sentiment = 0\n",
    "#         else:\n",
    "#             sentiment_score, label = sentiment_analysis(review_text[:max_length])\n",
    "#             if label == 'positive':\n",
    "#                 sentiment = 1\n",
    "#             elif label == 'neutral':\n",
    "#                 sentiment = 0\n",
    "#             elif label == 'negative':\n",
    "#                 sentiment = -1\n",
    "                \n",
    "#         sentiment_list.append(({'_id': review['_id']}, {'$set': {'sentiment': sentiment}}))\n",
    "        \n",
    "#         if count % 1000 == 0:\n",
    "#             update_requests = []\n",
    "\n",
    "#             for sentiment in sentiment_list:\n",
    "#                 update_requests.append(\n",
    "#                     UpdateOne(\n",
    "#                         sentiment[0],\n",
    "#                         sentiment[1]\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#             # Thực hiện bulk write\n",
    "#             result = review_collection_1.bulk_write(update_requests)\n",
    "#             sentiment_list = []\n",
    "            \n",
    "#         count += 1\n",
    "        \n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all User Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user_list = []\n",
    "\n",
    "# # for review_collection in review_collections:\n",
    "# #     progress_bar = tqdm(total=review_collection.count_documents({'User_id': {'$exists': True}}), desc='Getting users', position=0)\n",
    "\n",
    "# #     reviews = review_collection.find({'User_id': {'$exists': True}})\n",
    "\n",
    "# #     for review in reviews:\n",
    "# #         user_list.append(review['User_id'])\n",
    "# #         progress_bar.update(1)\n",
    "        \n",
    "# #     progress_bar.close()\n",
    "# user_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=users_collection.count_documents({'User_id': {'$exists': True}}), desc='Getting User id', position=0)\n",
    "\n",
    "# users = users_collection.find({'User_id': {'$exists': True}})\n",
    "\n",
    "# for user in users:\n",
    "#     user_list.append(user['User_id'])\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=book_collection.count_documents({'Title': {'$exists': True}}), desc='Getting titles', position=0)\n",
    "\n",
    "# books = book_collection.find({'Title': {'$exists': True}})\n",
    "\n",
    "# for book in books:\n",
    "#     title_list.append(book['Title'])\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_documents = [{'User_id': user_id} for user_id in user_list]\n",
    "\n",
    "# # Chèn các document mới vào collection\n",
    "# result = users_collection.insert_many(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.DataFrame(index=list(user_list), columns=title_list, dtype=int)\n",
    "# # print(df.memory_usage(index=True).sum())\n",
    "# # matrix = np.zeros((1000, 212404))\n",
    "# # memory_usage = matrix.nbytes\n",
    "\n",
    "# # # Chuyển đổi dung lượng bộ nhớ sang đơn vị GB\n",
    "# # memory_usage_gb = memory_usage / (1024**3)\n",
    "\n",
    "# selected_title = []\n",
    "# selected_books  = book_collection.find({'categories': {'$regex': 'fiction'}})\n",
    "# for book in selected_books:\n",
    "#     selected_title.append(book['Title'])\n",
    "# print(selected_title[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# progress_bar = tqdm(total=len(selected_title), desc='Getting user scores...', position=0)\n",
    "\n",
    "# # Tìm số sao theo từng title\n",
    "# for title in selected_title:\n",
    "#     title_and_users = [title]\n",
    "#     # Tìm trong 8 collection các review của quyển sách đó\n",
    "#     for review_collection in review_collections:\n",
    "#         found_reviews = review_collection.find({'Title': title})\n",
    "#         # Lấy thông tin của từng review đó\n",
    "#         for review in found_reviews:\n",
    "#             if 'User_id' in review:\n",
    "#                 user_score = {review['User_id']: review['review/score']}\n",
    "#                 title_and_users.append(user_score)\n",
    "#     user_item_list.append(title_and_users)\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# data = df_all\n",
    "# y = data['review/score']\n",
    "# X = data.drop('review/score', axis=1)\n",
    "# print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Tokenize các review\n",
    "# data['tokenized_title'] = data['Title'].apply(nltk.word_tokenize)\n",
    "\n",
    "# # Huấn luyện mô hình Word2Vec\n",
    "# model = Word2Vec(data['tokenized_title'], min_count=1)\n",
    "\n",
    "# # Lấy vector đại diện cho từ 'book'\n",
    "# vector = model.wv['book']\n",
    "\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # scaler = StandardScaler()\n",
    "# # X_train = scaler.fit_transform(X_train)\n",
    "# # X_test = scaler.transform(X_test)\n",
    "\n",
    "# # # Xây dựng mô hình\n",
    "# # model = Sequential()\n",
    "# # model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # model.add(Dense(64, activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # model.add(Dense(1))\n",
    "\n",
    "# # # Huấn luyện mô hình\n",
    "# # model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# # model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620425, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "                    #  chunksize=CHUNK_SIZE,)\n",
    "                    \n",
    "df_all.drop(df_all.columns[[0,1]], axis=1, inplace=True)\n",
    "df_all.dropna(subset=['User_id', 'Title', 'categories'], inplace=True)\n",
    "df_all.drop(['review/summary', 'review/text', 'authors', 'description', 'review/median_score', 'review/mode_score', 'sentiment'], axis=1, inplace=True)\n",
    "df_all.drop_duplicates(inplace = True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>User_id</th>\n",
       "      <th>review/score</th>\n",
       "      <th>categories</th>\n",
       "      <th>review/count</th>\n",
       "      <th>review/mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its only art if its well hung</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['comics graphic novels']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr seuss american icon</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['biography autobiography']</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dr seuss american icon</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['biography autobiography']</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dr seuss american icon</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['biography autobiography']</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dr seuss american icon</td>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['biography autobiography']</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title         User_id  review/score  \\\n",
       "0  its only art if its well hung   AVCGYZL8FQQTD           4.0   \n",
       "1         dr seuss american icon  A30TK6U7DNS82R           5.0   \n",
       "2         dr seuss american icon  A3UH4UZ4RSVO82           5.0   \n",
       "3         dr seuss american icon  A2MVUWT453QH61           4.0   \n",
       "4         dr seuss american icon  A22X4XUPKF66MR           4.0   \n",
       "\n",
       "                    categories  review/count  review/mean_score  \n",
       "0    ['comics graphic novels']           1.0           4.000000  \n",
       "1  ['biography autobiography']           9.0           4.555556  \n",
       "2  ['biography autobiography']           9.0           4.555556  \n",
       "3  ['biography autobiography']           9.0           4.555556  \n",
       "4  ['biography autobiography']           9.0           4.555556  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiction</th>\n",
       "      <th>religion</th>\n",
       "      <th>history</th>\n",
       "      <th>juvenile fiction</th>\n",
       "      <th>biography autobiography</th>\n",
       "      <th>business economics</th>\n",
       "      <th>computers</th>\n",
       "      <th>social science</th>\n",
       "      <th>juvenile nonfiction</th>\n",
       "      <th>science</th>\n",
       "      <th>...</th>\n",
       "      <th>study aids</th>\n",
       "      <th>games activities</th>\n",
       "      <th>house home</th>\n",
       "      <th>english language</th>\n",
       "      <th>united states</th>\n",
       "      <th>true crime</th>\n",
       "      <th>design</th>\n",
       "      <th>great britain</th>\n",
       "      <th>animals</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>perfect secretary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the spanish prisoner and the winslow boy two s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chicken soup for the fishermans soul fish tale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fiction  religion  history  juvenile fiction  biography autobiography  \\\n",
       "0        0         0        0                 0                        0   \n",
       "1        0         0        0                 0                        0   \n",
       "2        0         0        0                 0                        0   \n",
       "\n",
       "   business economics  computers  social science  juvenile nonfiction  \\\n",
       "0                   0          0               0                    0   \n",
       "1                   0          0               0                    0   \n",
       "2                   0          0               0                    0   \n",
       "\n",
       "   science  ...  study aids  games activities  house home  english language  \\\n",
       "0        0  ...           0                 0           0                 1   \n",
       "1        0  ...           0                 0           0                 0   \n",
       "2        0  ...           0                 0           0                 0   \n",
       "\n",
       "   united states  true crime  design  great britain  animals  \\\n",
       "0              0           0       0              0        0   \n",
       "1              0           0       0              0        0   \n",
       "2              0           0       0              0        0   \n",
       "\n",
       "                                               Title  \n",
       "0                                  perfect secretary  \n",
       "1  the spanish prisoner and the winslow boy two s...  \n",
       "2  chicken soup for the fishermans soul fish tale...  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tạo one hot encoding vector đối với top các category\n",
    "'''\n",
    "sample_size = 10000\n",
    "data = df_all.sample(sample_size)\n",
    "data.shape\n",
    "Categories = ['Fiction', 'Religion', 'History', 'Juvenile Fiction', 'Biography & Autobiography', 'Business & Economics', 'Computers', 'Social Science', 'Juvenile Nonfiction', 'Science', 'Education', 'Cooking', 'Sports & Recreation', 'Family & Relationships', 'Literary Criticism', 'Music', 'Medical', 'Art', 'Body, Mind & Spirit', 'Language Arts & Disciplines', 'Health & Fitness', 'Political Science', 'Psychology', 'Philosophy', 'Travel', 'Technology & Engineering', 'Self-Help', 'Poetry', 'Foreign Language Study', 'Crafts & Hobbies', 'Performing Arts', 'Reference', 'Mathematics', 'Comics & Graphic Novels', 'Nature', 'Architecture', 'Transportation', 'Law', 'Humor', 'Photography', 'American literature', 'Antiques & Collectibles', 'Drama', 'Bibles', 'Pets', 'Literary Collections', 'Young Adult Fiction', 'Games', 'Gardening', 'Study Aids', 'Games & Activities', 'House & Home', 'English language', 'United States', 'True Crime', 'Design', 'Great Britain', 'Animals']\n",
    "for index, category in enumerate(Categories):\n",
    "    category = category.lower()\n",
    "    category = remove_special_characters(category)\n",
    "    category = re.sub(r'\\s+', ' ', category)\n",
    "    Categories[index] = category\n",
    "\n",
    "title_set = set([row['Title'] for i, row in data.iterrows()])\n",
    "\n",
    "df_categories = pd.DataFrame(columns=Categories)\n",
    "df_categories = pd.DataFrame({col: [0] * len(title_set) for col in Categories})\n",
    "\n",
    "for i, title in enumerate(title_set):\n",
    "    df_categories.at[i, 'Title'] = title \n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    title = row['Title']\n",
    "    categories = row['categories'][2:-2]\n",
    "    \n",
    "    if categories in Categories:\n",
    "        df_categories.loc[df_categories['Title'] == title, categories] = 1\n",
    "        \n",
    "        \n",
    "df_categories.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tạo 3 dataframe tương ứng với thông tin sách, thông tin user, thông tin ratings\n",
    "'''\n",
    "df_combined = data.merge(df_categories, on='Title')\n",
    "df_combined.drop(['categories'], axis=1, inplace=True)\n",
    "df_combined.head(2)\n",
    "df_ratings = copy.deepcopy(df_combined)\n",
    "df_ratings.drop(['review/count', 'review/mean_score'], axis=1, inplace=True)\n",
    "\n",
    "# Tính tổng và trung bình review/score theo User_id\n",
    "df_user = df_combined.groupby('User_id')['review/score'].agg(['count', 'mean'])\n",
    "df_user.columns = ['count', 'mean']\n",
    "df_user.reset_index(inplace=True)\n",
    "# print(user_review_summary.head(2))\n",
    "\n",
    "# total_review_user = user_review_summary.loc[user_review_summary['total_review'] > 1, 'total_review']\n",
    "# print(total_review_user)\n",
    "df_book = df_combined.drop(['User_id', 'review/score'], axis=1, inplace=True)\n",
    "df_book = df_combined.drop(Categories, axis=1, inplace=True)\n",
    "df_book = df_combined.drop_duplicates('Title')\n",
    "\n",
    "# df_combined.drop(['review/count', 'review/mean_score'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>new_user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0160198NNRLD37TTP2P</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A08604952BQMNOFP9OIUX</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 User_id  count  mean  new_user_id\n",
       "0   A0160198NNRLD37TTP2P      1   4.0            0\n",
       "1  A08604952BQMNOFP9OIUX      1   5.0            1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set user id mới\n",
    "user_id_map = {user_id: i for i, user_id in enumerate(df_user['User_id'].unique())}\n",
    "df_user['new_user_id'] = df_user['User_id'].map(user_id_map)\n",
    "df_user.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22108\\2291317216.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_book['new_book_id'] = df_book['Title'].map(book_id_map)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>review/count</th>\n",
       "      <th>review/mean_score</th>\n",
       "      <th>new_book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan a russian soldiers story</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.639344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reiki energy medicine bringing healing touch i...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  review/count  \\\n",
       "0               afghanistan a russian soldiers story          61.0   \n",
       "1  reiki energy medicine bringing healing touch i...          16.0   \n",
       "\n",
       "   review/mean_score  new_book_id  \n",
       "0           4.639344            0  \n",
       "1           4.250000            1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set book id mới\n",
    "book_id_map = {book_id: i for i, book_id in enumerate(df_book['Title'].unique())}\n",
    "df_book['new_book_id'] = df_book['Title'].map(book_id_map)\n",
    "df_book.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>User_id</th>\n",
       "      <th>review/score</th>\n",
       "      <th>fiction</th>\n",
       "      <th>religion</th>\n",
       "      <th>history</th>\n",
       "      <th>juvenile fiction</th>\n",
       "      <th>biography autobiography</th>\n",
       "      <th>business economics</th>\n",
       "      <th>computers</th>\n",
       "      <th>...</th>\n",
       "      <th>true crime</th>\n",
       "      <th>design</th>\n",
       "      <th>great britain</th>\n",
       "      <th>animals</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>new_user_id</th>\n",
       "      <th>review/count</th>\n",
       "      <th>review/mean_score</th>\n",
       "      <th>new_book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghanistan a russian soldiers story</td>\n",
       "      <td>A14Z24E7W0HT84</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>369</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.639344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reiki energy medicine bringing healing touch i...</td>\n",
       "      <td>A2S94UYOSEJGB</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4450</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title         User_id  \\\n",
       "0               afghanistan a russian soldiers story  A14Z24E7W0HT84   \n",
       "1  reiki energy medicine bringing healing touch i...   A2S94UYOSEJGB   \n",
       "\n",
       "   review/score  fiction  religion  history  juvenile fiction  \\\n",
       "0           5.0        0         0        1                 0   \n",
       "1           5.0        0         0        0                 0   \n",
       "\n",
       "   biography autobiography  business economics  computers  ...  true crime  \\\n",
       "0                        0                   0          0  ...           0   \n",
       "1                        0                   0          0  ...           0   \n",
       "\n",
       "   design  great britain  animals  count  mean  new_user_id  review/count  \\\n",
       "0       0              0        0      1   5.0          369          61.0   \n",
       "1       0              0        0      1   5.0         4450          16.0   \n",
       "\n",
       "   review/mean_score  new_book_id  \n",
       "0           4.639344            0  \n",
       "1           4.250000            1  \n",
       "\n",
       "[2 rows x 67 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cập nhật user id và book id mới\n",
    "df_ratings = df_ratings.merge(df_user, on='User_id').merge(df_book, on='Title')\n",
    "df_ratings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22108\\3330095002.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_book.drop(['Title'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Xóa đi các cột Title và User_id cũ\n",
    "df_user.drop(['User_id'], axis=1, inplace=True)\n",
    "df_book.drop(['Title'], axis=1, inplace=True)\n",
    "df_ratings.drop(['Title', 'User_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chia data thành tập train và test\n",
    "def split_dataframe(df, holdout_fraction=0.1):\n",
    "  test = df.sample(frac=holdout_fraction, replace=False)\n",
    "  train = df[~df.index.isin(test.index)]\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a tf.SparseTensor of the Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rating_sparse_tensor(df_ratings):\n",
    "    indices = df_ratings[['new_user_id', 'new_book_id']].values\n",
    "    values = df_ratings['review/score'].values\n",
    "    return tf.SparseTensor(\n",
    "        indices=indices,\n",
    "        values=values,\n",
    "        dense_shape=[df_user.shape[0],\n",
    "                     df_book.shape[0]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mean_square_error(sparse_ratings, user_embeddings, book_embeddings):\n",
    "    predictions = tf.reduce_sum(\n",
    "        tf.gather(user_embeddings, sparse_ratings.indices[:, 0]) * \n",
    "        tf.gather(book_embeddings, sparse_ratings.indices[:, 1]),\n",
    "        axis=1\n",
    "    )\n",
    "    loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFModel(object):\n",
    "    def __init__(self, embedding_vars, loss, metrics=None):\n",
    "        self._embedding_vars = embedding_vars\n",
    "        self._loss = loss\n",
    "        self._metrics = metrics\n",
    "        self._embeddings = {k: None for k in embedding_vars}\n",
    "        self._session = None\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        return self._embeddings\n",
    "\n",
    "    def train(self, num_iterations=100, learning_rate=1.0, plot_results=True, optimizer=tf.keras.optimizers.SGD):\n",
    "        opt = optimizer(learning_rate)\n",
    "\n",
    "        local_init_op = tf.group(\n",
    "          tf.variables_initializer(opt.variables()),\n",
    "          tf.local_variables_initializer())\n",
    "        if self._session is None:\n",
    "            self._session = tf.compat.v1.Session()\n",
    "            with self._session.as_default():\n",
    "                self._session.run(tf.compat.v1.global_variables_initializer())\n",
    "                self._session.run(tf.compat.v1.tables_initializer())\n",
    "\n",
    "        local_init_op.run()\n",
    "        iterations = []\n",
    "        metrics = self._metrics or ({},)\n",
    "        metrics_vals = [defaultdict(list) for _ in self._metrics]\n",
    "\n",
    "        for i in range(num_iterations + 1):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss_val = self._loss()  # Đảm bảo `_loss` trả về một scalar\n",
    "            gradients = tape.gradient(loss_val, list(self._embedding_vars.values()))\n",
    "            opt.apply_gradients(zip(gradients, list(self._embedding_vars.values())))\n",
    "\n",
    "            _, results = self._session.run((loss_val, metrics))\n",
    "            if i % 10 == 0 or i == num_iterations:\n",
    "                print(\"\\r iteration %d: \" % i + \", \".join([\"%s=%f\" % (k, v) for r in results for k, v in r.items()]), end='')\n",
    "                iterations.append(i)\n",
    "                for metric_val, result in zip(metrics_vals, results):\n",
    "                    for k, v in result.items():\n",
    "                        metric_val[k].append(v)\n",
    "\n",
    "        for k, v in self._embedding_vars.items():\n",
    "            self._embeddings[k] = v.eval()\n",
    "\n",
    "        if plot_results:\n",
    "            num_subplots = len(metrics+1)\n",
    "            fig = plt.figure()\n",
    "            fig.set_size_inches(num_subplots * 10, 8)\n",
    "            for i, metric_vals in enumerate(metrics_vals):\n",
    "                ax = fig.add_subplot(1, num_subplots, i+1)\n",
    "                for k, v in metric_vals.items():\n",
    "                    ax.plot(iterations, v, label=k)\n",
    "                ax.set_xlim([1, num_iterations])\n",
    "                ax.legend()\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
    "  # Split the ratings DataFrame into train and test.\n",
    "  train_ratings, test_ratings = split_dataframe(ratings)\n",
    "  # SparseTensor representation of the train and test datasets.\n",
    "  A_train = build_rating_sparse_tensor(train_ratings)\n",
    "  A_test = build_rating_sparse_tensor(test_ratings)\n",
    "  # Initialize the embeddings using a normal distribution.\n",
    "  U = tf.Variable(tf.random.normal(\n",
    "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
    "  V = tf.Variable(tf.random.normal(\n",
    "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
    "  train_loss = sparse_mean_square_error(A_train, U, V)\n",
    "  test_loss = sparse_mean_square_error(A_test, U, V)\n",
    "  metrics = {\n",
    "      'train_error': train_loss,\n",
    "      'test_error': test_loss\n",
    "  }\n",
    "  embeddings = {\n",
    "      \"new_user_id\": U,\n",
    "      \"new_movie_id\": V\n",
    "  }\n",
    "  return CFModel(embeddings, train_loss, [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'variables_initializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m build_model(df_ratings, embedding_dim\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, init_stddev\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mtrain(num_iterations\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m10.\u001b[39m)\n",
      "Cell \u001b[1;32mIn[62], line 17\u001b[0m, in \u001b[0;36mCFModel.train\u001b[1;34m(self, num_iterations, learning_rate, plot_results, optimizer)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, num_iterations\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, plot_results\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD):\n\u001b[0;32m     14\u001b[0m     opt \u001b[39m=\u001b[39m optimizer(learning_rate)\n\u001b[0;32m     16\u001b[0m     local_init_op \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mgroup(\n\u001b[1;32m---> 17\u001b[0m       tf\u001b[39m.\u001b[39mvariables_initializer(opt\u001b[39m.\u001b[39mvariables()),\n\u001b[0;32m     18\u001b[0m       tf\u001b[39m.\u001b[39mlocal_variables_initializer())\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mSession()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'variables_initializer'"
     ]
    }
   ],
   "source": [
    "model = build_model(df_ratings, embedding_dim=30, init_stddev=0.5)\n",
    "model.train(num_iterations=1000, learning_rate=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "# from scipy.sparse import csr_matrix\n",
    "\n",
    "# users = data['User_id'].unique()\n",
    "# items = data['Title'].unique()\n",
    "\n",
    "# # Tạo từ điển ánh xạ giữa user hoặc item và chỉ mục của chúng trong mảng NumPy\n",
    "# user_dict = {user: i for i, user in enumerate(users)}\n",
    "# item_dict = {item: i for i, item in enumerate(items)}\n",
    "# average_rating_dict = {row['Title']: row['review/mean_score'] for i, row in data.iterrows()}\n",
    "\n",
    "# # Xây dựng ma trận người dùng-sản phẩm\n",
    "# user_item_matrix = csr_matrix((len(users), len(items)), dtype=float)\n",
    "\n",
    "# # Điền giá trị đánh giá vào ma trận\n",
    "# for _, row in data.iterrows():\n",
    "#     user_index = user_dict.get(row['User_id'])\n",
    "#     item_index = item_dict.get(row['Title'])\n",
    "#     user_item_matrix[user_index, item_index] = row['review/score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9302\n",
      "6195\n",
      "6195\n"
     ]
    }
   ],
   "source": [
    "# print(len(user_dict))\n",
    "# print(len(item_dict))\n",
    "# print(len(average_rating_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t5.0\n",
      "  (1, 1)\t5.0\n",
      "  (2, 2)\t5.0\n",
      "  (3, 3)\t4.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t5.0\n",
      "  (6, 6)\t5.0\n",
      "  (7, 7)\t4.0\n",
      "  (8, 8)\t5.0\n",
      "  (9, 9)\t4.0\n"
     ]
    }
   ],
   "source": [
    "# print(user_item_matrix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poland 1939 the birth of blitzkrieg campaign: 4.769230769230769\n",
      "the other boleyn girl: 4.28584729981378\n",
      "brave new world: 4.235266159695818\n",
      "the 21st century investor investing for your childs college education: 4.0\n",
      "day of reckoning the massacre at columbine high school: 1.2857142857142858\n",
      "the day after roswell the truth exposed after fifty years: 3.898989898989899\n",
      "the kids campfire book: 4.818181818181818\n",
      "invisible monsters a novel: 4.213333333333333\n",
      "anna karenina television tiein edition signet classics: 4.461538461538462\n",
      "the burgess animal book for children: nan\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# # In ra cặp key-value đầu tiên của từ điển\n",
    "# for key, value in average_rating_dict.items():\n",
    "#     if count == 10:\n",
    "#         break\n",
    "#     print(f\"{key}: {value}\")\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UBCF:\\\n",
    "Tính toán giá trị rating dự đoán của toàn bộ sách đối với một user target.\\\n",
    "Tìm top k=10 sách có giá trị rating cao nhất và đề xuất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10\n",
    "\n",
    "# r_mean = list(average_rating_dict.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giá trị sentiment:\n",
    "- Positive: 1\n",
    "- Neutral: 0\n",
    "- Negative: -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Sentiment'] = data['review/text'].apply(lambda review: sentiment_analysis(review[:512]))\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>review/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1127126</th>\n",
       "      <td>poland 1939 the birth of blitzkrieg campaign</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134979</th>\n",
       "      <td>the other boleyn girl</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359255</th>\n",
       "      <td>brave new world</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234538</th>\n",
       "      <td>the 21st century investor investing for your c...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439700</th>\n",
       "      <td>day of reckoning the massacre at columbine hig...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Title  review/score\n",
       "1127126       poland 1939 the birth of blitzkrieg campaign           5.0\n",
       "134979                               the other boleyn girl           5.0\n",
       "359255                                     brave new world           5.0\n",
       "3234538  the 21st century investor investing for your c...           4.0\n",
       "2439700  day of reckoning the massacre at columbine hig...           1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.drop(['User_id', 'review/summary', 'review/text', 'authors', 'categories', 'description', 'review/count', 'review/mean_score', 'review/median_score', 'review/mode_score', 'sentiment'], axis=1, inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# # Chuẩn bị dữ liệu đầu vào\n",
    "# titles = data['Title']\n",
    "\n",
    "# # Huấn luyện mô hình Word2Vec\n",
    "# model_w2v = Word2Vec(sentences=titles.str.lower().str.split(), vector_size=1, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Áp dụng Word2Vec vào mỗi tiêu đề trong dataframe\n",
    "# data['Title_Embedding'] = data['Title'].apply(lambda title: model_w2v.wv[str(title).lower().split()])\n",
    "# data['Title_Embedding'] = data['Title_Embedding'].apply(lambda embedding: np.array(embedding).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>review/score</th>\n",
       "      <th>Title_Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1127126</th>\n",
       "      <td>poland 1939 the birth of blitzkrieg campaign</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[-0.6957804, -0.060058255, 7.994793, 1.5919697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134979</th>\n",
       "      <td>the other boleyn girl</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[7.994793, 1.9857986, 0.7660866, 1.9958899]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359255</th>\n",
       "      <td>brave new world</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.782776, 5.6055765, 4.4388776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234538</th>\n",
       "      <td>the 21st century investor investing for your c...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[7.994793, -0.22492482, 1.4023304, -0.08699676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439700</th>\n",
       "      <td>day of reckoning the massacre at columbine hig...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.2522843, 7.602487, -0.24409083, 7.994793, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184295</th>\n",
       "      <td>the day after roswell the truth exposed after ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[7.994793, 1.2522843, 1.6834279, -0.13873205, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062932</th>\n",
       "      <td>the kids campfire book</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[7.994793, 0.08365127, -0.7417103, 6.460596]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041107</th>\n",
       "      <td>invisible monsters a novel</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.11386006, 0.98627627, 7.5766444, 3.8376641]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772972</th>\n",
       "      <td>anna karenina television tiein edition signet ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.07227795, 0.65132666, 0.7224068, 0.3157225,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528193</th>\n",
       "      <td>the burgess animal book for children</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[7.994793, 0.057889283, -0.06820843, 6.460596,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359439</th>\n",
       "      <td>brave new world</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[0.782776, 5.6055765, 4.4388776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776397</th>\n",
       "      <td>a treasury of polish aphorisms</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[7.5766444, -0.011404766, 7.602487, -0.676352,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776266</th>\n",
       "      <td>george orwell animal farmnineteen eightyfour</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[1.4786904, -0.001132373, -0.06820843, 0.19463...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383045</th>\n",
       "      <td>sawyers crossing</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[1.0009639, -0.07176844]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041255</th>\n",
       "      <td>the castle of crossed destinies</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[7.994793, 1.1627675, 7.602487, -0.1807647, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599844</th>\n",
       "      <td>hurricane hex charmed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0.2597214, -0.87995964, -0.7297379]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564142</th>\n",
       "      <td>of mice and men</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[7.602487, 1.4221145, 8.663713, 1.4675212]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642030</th>\n",
       "      <td>midnights children</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[-0.29859564, 1.8128812]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242533</th>\n",
       "      <td>the very virile viking</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[7.994793, -0.24859513, 0.6708194, 0.88274646]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389823</th>\n",
       "      <td>seven pillars of wisdom a triumph</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.6567646, 0.87894434, 7.602487, 1.7742407, 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Title  review/score  \\\n",
       "1127126       poland 1939 the birth of blitzkrieg campaign           5.0   \n",
       "134979                               the other boleyn girl           5.0   \n",
       "359255                                     brave new world           5.0   \n",
       "3234538  the 21st century investor investing for your c...           4.0   \n",
       "2439700  day of reckoning the massacre at columbine hig...           1.0   \n",
       "3184295  the day after roswell the truth exposed after ...           5.0   \n",
       "3062932                             the kids campfire book           5.0   \n",
       "3041107                         invisible monsters a novel           4.0   \n",
       "772972   anna karenina television tiein edition signet ...           5.0   \n",
       "1528193               the burgess animal book for children           4.0   \n",
       "359439                                     brave new world           5.0   \n",
       "776397                      a treasury of polish aphorisms           5.0   \n",
       "776266        george orwell animal farmnineteen eightyfour           3.0   \n",
       "2383045                                   sawyers crossing           4.0   \n",
       "1041255                    the castle of crossed destinies           3.0   \n",
       "2599844                              hurricane hex charmed           4.0   \n",
       "1564142                                    of mice and men           5.0   \n",
       "1642030                                 midnights children           4.0   \n",
       "1242533                             the very virile viking           2.0   \n",
       "2389823                  seven pillars of wisdom a triumph           5.0   \n",
       "\n",
       "                                           Title_Embedding  \n",
       "1127126  [-0.6957804, -0.060058255, 7.994793, 1.5919697...  \n",
       "134979         [7.994793, 1.9857986, 0.7660866, 1.9958899]  \n",
       "359255                    [0.782776, 5.6055765, 4.4388776]  \n",
       "3234538  [7.994793, -0.22492482, 1.4023304, -0.08699676...  \n",
       "2439700  [1.2522843, 7.602487, -0.24409083, 7.994793, -...  \n",
       "3184295  [7.994793, 1.2522843, 1.6834279, -0.13873205, ...  \n",
       "3062932       [7.994793, 0.08365127, -0.7417103, 6.460596]  \n",
       "3041107     [0.11386006, 0.98627627, 7.5766444, 3.8376641]  \n",
       "772972   [0.07227795, 0.65132666, 0.7224068, 0.3157225,...  \n",
       "1528193  [7.994793, 0.057889283, -0.06820843, 6.460596,...  \n",
       "359439                    [0.782776, 5.6055765, 4.4388776]  \n",
       "776397   [7.5766444, -0.011404766, 7.602487, -0.676352,...  \n",
       "776266   [1.4786904, -0.001132373, -0.06820843, 0.19463...  \n",
       "2383045                           [1.0009639, -0.07176844]  \n",
       "1041255  [7.994793, 1.1627675, 7.602487, -0.1807647, -0...  \n",
       "2599844               [0.2597214, -0.87995964, -0.7297379]  \n",
       "1564142         [7.602487, 1.4221145, 8.663713, 1.4675212]  \n",
       "1642030                           [-0.29859564, 1.8128812]  \n",
       "1242533     [7.994793, -0.24859513, 0.6708194, 0.88274646]  \n",
       "2389823  [1.6567646, 0.87894434, 7.602487, 1.7742407, 7...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(['Title'], axis=1, inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1127126    2.514557\n",
       "134979     3.185642\n",
       "359255     3.609077\n",
       "3234538    2.676221\n",
       "2439700    2.656196\n",
       "Name: Title_Embedding_Mean, dtype: float32"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['Title_Embedding_Sum'] = data['Title_Embedding'].apply(lambda embedding: np.sum(embedding))\n",
    "# data['Title_Embedding_Sum'].head()\n",
    "# data['Title_Embedding_Mean'] = data['Title_Embedding'].apply(lambda embedding: np.mean(embedding))\n",
    "# data['Title_Embedding_Mean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Sentiment'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[39m# Chia dữ liệu thành features (X) và target (y)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X \u001b[39m=\u001b[39m data[[\u001b[39m'\u001b[39m\u001b[39mSentiment\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTitle_Embedding_Sum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTitle_Embedding_Mean\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      7\u001b[0m y \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mreview/score\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[39m# # Chia dữ liệu thành tập huấn luyện và tập kiểm tra\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39m_get_indexer_strict(key, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Sentiment'] not in index\""
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Chia dữ liệu thành features (X) và target (y)\n",
    "# X = data[['Sentiment', 'Title_Embedding_Sum', 'Title_Embedding_Mean']]\n",
    "# y = data['review/score']\n",
    "\n",
    "# # # Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Khởi tạo mô hình mạng nơ-ron\n",
    "# model = tf.keras.models.Sequential([\n",
    "#       tf.keras.layers.Flatten(),\n",
    "#       tf.keras.layers.Dense(128, activation='relu'),\n",
    "#       tf.keras.layers.Dropout(0.2),\n",
    "#       tf.keras.layers.Dense(5, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile mô hình với loss function là 'mean_squared_error' và optimizer là 'adam'\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Huấn luyện mô hình trên tập huấn luyện\n",
    "# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Đánh giá hiệu suất của mô hình trên tập kiểm tra\n",
    "# mse = model.evaluate(X_test, y_test)\n",
    "# print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5765\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Khởi tạo mô hình KNN với k = 3 (có thể điều chỉnh giá trị k tùy ý)\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# # Huấn luyện mô hình trên tập huấn luyện\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# # Dự đoán trên tập kiểm tra\n",
    "# y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# # Đánh giá hiệu suất của mô hình bằng độ chính xác\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 nearest neighbors for book 1:\n",
      "Neighbor 1: gods little acre (review score: 2.0)\n",
      "Neighbor 2: recipes for longer life (review score: 5.0)\n",
      "Neighbor 3: the hobbit (review score: 5.0)\n",
      "Neighbor 4: serpents walk (review score: 5.0)\n",
      "Neighbor 5: dharma beads making and using your own buddhist malas (review score: 5.0)\n",
      "Neighbor 6: child behavior (review score: 4.0)\n",
      "Neighbor 7: why revival tarries (review score: 4.0)\n",
      "Neighbor 8: crucible the (review score: 5.0)\n",
      "Neighbor 9: small dolls of the 40s and 50s identification and value guide (review score: 5.0)\n",
      "Neighbor 10: the golden compass (review score: 4.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# new_title = 'harry potter and the chamber of secrets'\n",
    "\n",
    "# new_title_embedding = model_w2v.wv[str(new_title).lower().split()]\n",
    "# new_title_embedding = np.array(new_title_embedding).flatten()\n",
    "# new_title_embedding_sum = np.sum(new_title_embedding)\n",
    "# new_title_embedding_mean = np.mean(new_title_embedding)\n",
    "\n",
    "# new_review = 'This book is great'\n",
    "# new_sentiment = sentiment_analysis(new_review)\n",
    "\n",
    "# distances, indices = knn_model.kneighbors([[new_sentiment, new_title_embedding_sum, new_title_embedding_mean]])\n",
    "\n",
    "# for i, book_index in enumerate(indices):\n",
    "#     print(f\"Top 10 nearest neighbors for book {i + 1}:\")\n",
    "#     for j, neighbor_index in enumerate(book_index):\n",
    "#         neighbor_title = data.iloc[neighbor_index]['Title']\n",
    "#         neighbor_score = y_train.iloc[neighbor_index]\n",
    "#         print(f\"Neighbor {j + 1}: {neighbor_title} (review score: {neighbor_score})\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
