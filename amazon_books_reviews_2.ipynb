{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import getpass\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import chain\n",
    "import datetime\n",
    "from fractions import Fraction\n",
    "from collections import Counter\n",
    "from pymongo import UpdateOne\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "import dask.dataframe as dd\n",
    "import html\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath_book = \"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\books_data.csv\"\n",
    "# filepath = \"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\Books_rating.csv\"\n",
    "\n",
    "# df_book = pd.read_csv(filepath_book)\n",
    "# df_review = pd.read_csv(filepath)\n",
    "\n",
    "password = 'Ngthiennhan2002.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_index = 0\n",
    "# chunk_size = 375000\n",
    "\n",
    "# for i in range(1, 9):\n",
    "#     end_index = start_index + chunk_size\n",
    "#     df_chunk = df_review.iloc[start_index:end_index]\n",
    "#     file_name = f\"D:\\\\University\\\\Môn học\\\\Ứng dụng Big Data\\\\Project\\\\Datasets\\\\Amazon Books Reviews\\\\Books_rating_{i}_375k.csv\"\n",
    "#     df_chunk.to_csv(file_name, index=False)\n",
    "#     start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cryptography\\x509\\base.py:594: CryptographyDeprecationWarning: Parsed a negative serial number, which is disallowed by RFC 5280.\n",
      "  return rust_x509.load_der_x509_certificate(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "212404\n"
     ]
    }
   ],
   "source": [
    "API_1 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.mhlvibl.mongodb.net/'\n",
    "# API_1 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.yl3o8ez.mongodb.net/'\n",
    "API_2 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.bzvhw41.mongodb.net/'\n",
    "API_3 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.jrdv2e2.mongodb.net/'\n",
    "API_4 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.tq84xea.mongodb.net/'\n",
    "API_5 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.i2p6hb8.mongodb.net/'\n",
    "API_6 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.ppuo86b.mongodb.net/'\n",
    "API_7 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.xjtimov.mongodb.net/'\n",
    "API_8 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.5ihto1h.mongodb.net/'\n",
    "API_items = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.2qutpiu.mongodb.net/'\n",
    "\n",
    "try:\n",
    "    # Create two MongoDB clients using MongoClient with two APIs\n",
    "    client_1 = MongoClient(API_1)\n",
    "    client_2 = MongoClient(API_2)\n",
    "    client_3 = MongoClient(API_3)\n",
    "    client_4 = MongoClient(API_4)\n",
    "    client_5 = MongoClient(API_5)\n",
    "    client_6 = MongoClient(API_6)\n",
    "    client_7 = MongoClient(API_7)\n",
    "    client_8 = MongoClient(API_8)\n",
    "    client_items = MongoClient(API_items)\n",
    "    \n",
    "    # Variables to save names of databases and collections\n",
    "    database_name = 'db'\n",
    "    review_collection_name = 'Reviews'\n",
    "    book_collection_name = 'Books'\n",
    "    author_collection_name = 'Authors'\n",
    "    categories_collection_name = 'Categories'\n",
    "    temp_collection_name = 'Temp'\n",
    "    user_collection_name = 'Users'\n",
    "\n",
    "    db_1 = client_1[database_name]\n",
    "    review_collection_1 = db_1[review_collection_name]\n",
    "\n",
    "    db_2 = client_2[database_name]\n",
    "    review_collection_2 = db_2[review_collection_name]\n",
    "    \n",
    "    db_3 = client_3[database_name]\n",
    "    review_collection_3 = db_3[review_collection_name]\n",
    "\n",
    "    db_4 = client_4[database_name]\n",
    "    review_collection_4 = db_4[review_collection_name]\n",
    "    \n",
    "    db_5 = client_5[database_name]\n",
    "    review_collection_5 = db_5[review_collection_name]\n",
    "\n",
    "    db_6 = client_6[database_name]\n",
    "    review_collection_6 = db_6[review_collection_name]\n",
    "    \n",
    "    db_7 = client_7[database_name]\n",
    "    review_collection_7 = db_7[review_collection_name]\n",
    "\n",
    "    db_8 = client_8[database_name]\n",
    "    review_collection_8 = db_8[review_collection_name]\n",
    "    \n",
    "    db_items = client_items[database_name]\n",
    "    book_collection = db_items[book_collection_name]\n",
    "    author_collection = db_items[author_collection_name]\n",
    "    categories_collection = db_items[categories_collection_name]\n",
    "    temp_collection = db_items[temp_collection_name]\n",
    "    users_collection = db_items[user_collection_name]\n",
    "except:\n",
    "    print(\"Incorrect password or cannot connect to MongoDB at this time\")\n",
    "    \n",
    "# Test if the documents are read successfully\n",
    "print(review_collection_1.count_documents({}))\n",
    "print(review_collection_2.count_documents({}))\n",
    "print(review_collection_3.count_documents({}))\n",
    "print(review_collection_4.count_documents({}))\n",
    "print(review_collection_5.count_documents({}))\n",
    "print(review_collection_6.count_documents({}))\n",
    "print(review_collection_7.count_documents({}))\n",
    "print(review_collection_8.count_documents({}))\n",
    "print(book_collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lengths of collections\n",
    "total_book = book_collection.count_documents({})\n",
    "total_review = 375000\n",
    "\n",
    "# Convert book collection into pandas DataFrame (with progress bar)\n",
    "# with tqdm(total=total_book, desc='Converting book collection') as pbar:\n",
    "#     df_book = pd.DataFrame(list(book_collection.find()))\n",
    "#     pbar.update(len(df_book))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 1') as pbar:\n",
    "#     df_review_1 = pd.DataFrame(list(review_collection_1.find()))\n",
    "#     pbar.update(len(df_review_1))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 2') as pbar:\n",
    "#     df_review_2 = pd.DataFrame(list(review_collection_2.find()))\n",
    "#     pbar.update(len(df_review_2))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 3') as pbar:\n",
    "#     df_review_3 = pd.DataFrame(list(review_collection_3.find()))\n",
    "#     pbar.update(len(df_review_3))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 4') as pbar:\n",
    "#     df_review_4 = pd.DataFrame(list(review_collection_4.find()))\n",
    "#     pbar.update(len(df_review_4))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 5') as pbar:\n",
    "#     df_review_5 = pd.DataFrame(list(review_collection_5.find()))\n",
    "#     pbar.update(len(df_review_5))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 6') as pbar:\n",
    "#     df_review_6 = pd.DataFrame(list(review_collection_6.find()))\n",
    "#     pbar.update(len(df_review_6))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 7') as pbar:\n",
    "#     df_review_7 = pd.DataFrame(list(review_collection_7.find()))\n",
    "#     pbar.update(len(df_review_7))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 8') as pbar:\n",
    "#     df_review_8 = pd.DataFrame(list(review_collection_8.find()))\n",
    "#     pbar.update(len(df_review_8))\n",
    "#     pbar.close()\n",
    "\n",
    "# print(\"--- Finished converting to DataFrame ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two df_review_1 and df_review_2\n",
    "# with tqdm(total=total_review, desc='Combining 2 review collections') as pbar:\n",
    "#     df_review = pd.concat([df_review_1, df_review_2, df_review_3, df_review_4, df_review_5, df_review_6, df_review_7, df_review_8])\n",
    "#     pbar.update(len(df_review))\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Title\\\n",
    "Missing values: 1 book thiếu Title -> điền thủ công bằng cách vào Link\\\n",
    "Noises: Chuyển chuỗi về kí tự thường và loại bỏ các kí tự đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count = 0\n",
    "\n",
    "# # for temp in temp_collection.find():\n",
    "# #     if temp['field'] == 'book_title':\n",
    "# #         temp_id = temp['_id']\n",
    "# #         count = int(temp['value'])\n",
    "# #         break\n",
    "\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Title', position=0)\n",
    "\n",
    "# title_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     title = book['Title']\n",
    "#     if type(title) is not str: \n",
    "#         title = str(title)\n",
    "#     title = title.lower()\n",
    "#     title = remove_special_characters(title)\n",
    "#     title = re.sub(r'\\s+', ' ', title)\n",
    "#     title_list.append(({'_id': book['_id']}, {'$set': {'Title': title}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for title in title_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             title[0],\n",
    "#             title[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Description\n",
    "- Missing values: Bỏ qua\n",
    "- Noises: Chuyển chuỗi về chữ thường và loại bỏ các kí tự đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Description', position=0)\n",
    "\n",
    "# description_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     if 'description' in book:\n",
    "#         description = str(book['description'])\n",
    "#         description = description.lower()\n",
    "#         description = remove_special_characters(description)\n",
    "#         description = re.sub(r'\\s+', ' ', description)\n",
    "#         description_list.append(({'_id': book['_id']}, {'$set': {'description': description}}))\n",
    "#     # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for description in description_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             description[0],\n",
    "#             description[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Authors và Categories\n",
    "- Missing values: Bỏ qua\n",
    "- Tạo danh sách chuỗi các tác giả\n",
    "- Noises: Chuyển chuỗi về chữ thường, loại bỏ các kí tự đặc biệt và xử lý các biến thể tên tác giả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = []\n",
    "# categories = []\n",
    "\n",
    "# for author in author_collection.find():\n",
    "#     author_id = author['_id']\n",
    "#     authors.append(({'_id': author_id}, {'$set': {'Value': author['Value']}}))\n",
    "    \n",
    "# for category in categories_collection.find():\n",
    "#     category_id = category['_id']\n",
    "#     categories.append(({'_id': category_id}, {'$set': {'Value': category['Value']}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=author_collection.count_documents({}), desc='Preprocessing Authors\\'s collection', position=0)\n",
    "\n",
    "# author_list = []\n",
    "\n",
    "# for author in author_collection.find():\n",
    "#     author_name = author['Value']\n",
    "#     if author_name:\n",
    "#         author_name = author_name.lower()\n",
    "#         author_name = remove_special_characters(author_name)\n",
    "#         author_name = re.sub(r'\\s+', ' ', author_name)\n",
    "#         author_list.append(({'_id': author['_id']}, {'$set': {'Value': author_name}}))\n",
    "#     # author_list.append({'_id': temp_id}, {'$set': {'Value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for author in author_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             {'_id': author[0]['_id']},\n",
    "#             {'$set': {'Value': author[1]['$set']['Value']}},\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = author_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=categories_collection.count_documents({}), desc='Preprocessing Categories\\'s collection', position=0)\n",
    "\n",
    "# category_list = []\n",
    "\n",
    "# for category in categories_collection.find():\n",
    "#     category_name = category['Value']\n",
    "#     category_name = category_name.lower()\n",
    "#     category_name = remove_special_characters(category_name)\n",
    "#     category_name = re.sub(r'\\s+', ' ', category_name)\n",
    "#     category_list.append(({'_id': category['_id']}, {'$set': {'Value': category_name}}))\n",
    "#     # temp_collection.update_one({'_id': temp_id}, {'$set': {'Value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for category in category_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             category[0],\n",
    "#             category[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = categories_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Publisher\n",
    "- Missing values: Bỏ qua\n",
    "- Noises: Loại bỏ kí tự đặc biệt, chuyển từ chữ hoa sang chữ thường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Publisher', position=0)\n",
    "\n",
    "# publisher_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     if 'publisher' in book:\n",
    "#         publisher = book['publisher']\n",
    "#         # print(publisher)\n",
    "#         if type(publisher) is int:\n",
    "#             publisher = str(publisher)\n",
    "#         publisher = publisher.lower()\n",
    "#         publisher = remove_special_characters(publisher)\n",
    "#         publisher = re.sub(r'\\s+', ' ', publisher)\n",
    "#         publisher_list.append(({'_id': book['_id']}, {'$set': {'publisher': publisher}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for publisher in publisher_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             publisher[0],\n",
    "#             publisher[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = book_collection.bulk_write(update_requests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường publishedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# def convert_to_datetime(date_str):\n",
    "#     formats = ['%Y', '%Y-%m', '%m-%Y', '%Y-%m-%d', '%m-%d-%Y', '%Y/%m/%d', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M', '%m/%d/%Y', '%d/%m/%Y']\n",
    "#     if isinstance(date_str, datetime):\n",
    "#         return date_str\n",
    "    \n",
    "#     if isinstance(date_str, int):\n",
    "#         date_str = str(date_str)\n",
    "        \n",
    "#     for fmt in formats:\n",
    "#         try:\n",
    "#             date_obj = datetime.strptime(date_str, fmt)\n",
    "#             if fmt.count('%') == 1:\n",
    "#                 return date_obj.strftime('%Y')\n",
    "#             elif fmt.count('%') == 2:\n",
    "#                 return date_obj.strftime('%Y-%m')\n",
    "#             else:\n",
    "#                 return date_obj.strftime('%Y-%m-%d')\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "#     return None\n",
    "    \n",
    "# count = 0\n",
    "\n",
    "# a = []\n",
    "\n",
    "# progress_bar = tqdm(total=total_book-count, desc='Preprocessing book collection\\'s Published Date', position=0)\n",
    "\n",
    "# for book in book_collection.find().skip(count):\n",
    "#     if 'publishedDate' in book:\n",
    "#         publishedDate = book['publishedDate']\n",
    "#         if publishedDate:\n",
    "#             try:\n",
    "#                 publishedDate = convert_to_datetime(publishedDate)\n",
    "#                 # book_collection.update_one({'_id': book['_id']}, {'$set': {'publishedDate': publishedDate}})\n",
    "#                 a.append(({'_id': book['_id']}, {'$set': {'publishedDate': publishedDate}}))\n",
    "#             except ValueError:\n",
    "#                 print(\"Error\")\n",
    "#         count += 1\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# published_date_list = a\n",
    "\n",
    "# for published_date in published_date_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             published_date[0],\n",
    "#             published_date[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Title (của Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_collections = [review_collection_1,\n",
    "                     review_collection_2,\n",
    "                     review_collection_3,\n",
    "                     review_collection_4,\n",
    "                     review_collection_5,\n",
    "                     review_collection_6,\n",
    "                     review_collection_7,\n",
    "                     review_collection_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, review_collection in enumerate(review_collections):\n",
    "    \n",
    "#     print(f'--- Preprocessing title of review collection {index + 1} ---')\n",
    "#     title_list = []\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review collection\\'s Title', position=0)\n",
    "    \n",
    "#     for index_2, review in enumerate(review_collection.find()):\n",
    "#         if 'Title' in review:\n",
    "#             title = review['Title']\n",
    "#             if type(title) is not str: \n",
    "#                 title = str(title)\n",
    "#             # title = title.lower()\n",
    "#             # title = remove_special_characters(title)\n",
    "#             title = re.sub(r'\\s+', ' ', title)\n",
    "#             title_list.append(({'_id': review['_id']}, {'$set': {'Title': title}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()\n",
    "    \n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     update_requests = []\n",
    "    \n",
    "#     for title in title_list:\n",
    "#         update_requests.append(\n",
    "#             UpdateOne(\n",
    "#                 title[0],\n",
    "#                 title[1],\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     print(update_requests[:5])\n",
    "\n",
    "#     # # Thực hiện bulk write\n",
    "#     # result = review_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường review/count và trường review/average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sử dụng defaultdict để tổ chức dữ liệu theo 'Title'\n",
    "# title_data = defaultdict(list)\n",
    "\n",
    "# # Thu thập dữ liệu từ các collection\n",
    "# for review_collection in review_collections:\n",
    "#     documents = review_collection.find({'review/score': {'$exists': True}})\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review scores', position=0)\n",
    "    \n",
    "#     for document in documents:\n",
    "#         if 'Title' in document:\n",
    "#             title = document['Title']\n",
    "#             title_data[title].append(document['review/score'])\n",
    "#             progress_bar.update(1)\n",
    "\n",
    "#     progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=len(title_data), desc='Calculating review scores', position=0)\n",
    "\n",
    "# new_book_scores_list = list()\n",
    "\n",
    "# for title, scores_list in title_data.items():\n",
    "#     count_reviews = len(scores_list)\n",
    "#     if scores_list:\n",
    "#         mean_score = statistics.mean(scores_list)\n",
    "#         median_score = statistics.median(scores_list)\n",
    "#         mode_score = statistics.mode(scores_list)\n",
    "#     else:\n",
    "#         mean_score, median_score, mode_score = 0, 0, 0\n",
    "        \n",
    "#     book_score_info = ({'Title': title}, {'$set': {'review/mean_score': mean_score, \n",
    "#                                                        'review/median_score': median_score,\n",
    "#                                                        'review/mode_score': mode_score,\n",
    "#                                                        'review/count': count_reviews}})\n",
    "#     new_book_scores_list.append((book_score_info))\n",
    "\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for book in new_book_scores_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             book[0],\n",
    "#             book[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews\n",
    "\n",
    "- Noises: Đã thực hiện ở trên\n",
    "- Missing values: Fill theo 'Id' nếu có 'Id' nào chứa Title đó. nếu không có để trống"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sử dụng defaultdict để tổ chức dữ liệu theo 'Title'\n",
    "# id_data = defaultdict(list)\n",
    "\n",
    "# progress_bar = tqdm(total=len(review_collections), desc='Preprocessing miss Review Titles', position=0)\n",
    "\n",
    "# # Thu thập dữ liệu từ các collection\n",
    "# for review_collection in review_collections:\n",
    "#     documents = review_collection.find({})\n",
    "    \n",
    "#     null_titles = review_collection.find({'Title': {'$exists': False}})\n",
    "    \n",
    "#     for document in null_titles:\n",
    "#         existed_titles_by_id_1 = review_collection_1.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_2 = review_collection_2.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_3 = review_collection_3.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_4 = review_collection_4.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_5 = review_collection_5.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_6 = review_collection_6.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_7 = review_collection_7.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_8 = review_collection_8.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "        \n",
    "#         for existed_review in existed_titles_by_id_1:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_2:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_3:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_4:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_5:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_6:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_7:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_8:\n",
    "#             print(existed_review)\n",
    "    \n",
    "#     progress_bar.update(1)\n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     # update_requests = []\n",
    "\n",
    "#     # for published_date in published_date_list:\n",
    "#     #     update_requests.append(\n",
    "#     #         UpdateOne(\n",
    "#     #             published_date[0],\n",
    "#     #             published_date[1],\n",
    "#     #         )\n",
    "#     #     )\n",
    "        \n",
    "#     # print(update_requests[:5])\n",
    "\n",
    "#     # # Thực hiện bulk write\n",
    "#     # result = review_collection.bulk_write(update_requests)\n",
    "\n",
    "# progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết luận: Không có Title bị khuyết nào có thể được điền thông qua chỉ số Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường review/helpfulness\n",
    "- Chuyển từ dạng a/b thành dạng float\n",
    "- Thay missing values = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, review_collection in enumerate(review_collections):\n",
    "    \n",
    "#     print(f'--- Preprocessing title of review collection {index + 1} ---')\n",
    "#     helpfulness_list = []\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review collection\\'s Helpfulness', position=0)\n",
    "    \n",
    "#     for index_2, review in enumerate(review_collection.find()):\n",
    "#         if 'review/helpfulness' in review:\n",
    "#             helpfulness_string = review['review/helpfulness']\n",
    "#             helpfulness_string = helpfulness_string.split('/')\n",
    "#             numerator = int(helpfulness_string[0])\n",
    "#             denominator = int(helpfulness_string[1])\n",
    "            \n",
    "#             # Nếu mẫu bằng 0\n",
    "#             if denominator == 0.0 or numerator == 0.0:\n",
    "#                 helpfulness_value = 0.0\n",
    "#             else:\n",
    "#                 helpfulness_value = numerator / denominator\n",
    "#                 if helpfulness_value > 1.0: # Xử lý các outlier lớn hơn 1\n",
    "#                     helpfulness_value = 1.0\n",
    "#         else:\n",
    "#             helpfulness_value = 0.0\n",
    "\n",
    "#         helpfulness_list.append(({'_id': review['_id']}, {'$set': {'review/helpfulness': helpfulness_value}}))\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()\n",
    "    \n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     update_requests = []\n",
    "    \n",
    "#     for helpfulness in helpfulness_list:\n",
    "#         update_requests.append(\n",
    "#             UpdateOne(\n",
    "#                 helpfulness[0],\n",
    "#                 helpfulness[1],\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     print(update_requests[:5])\n",
    "\n",
    "#     # Thực hiện bulk write\n",
    "#     result = review_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Authors và Categories\n",
    "- Chuyển về dạng list các tác giả và các danh mục\n",
    "- Chuyển về format chữ thường, loại bỏ kí tự đặc biệt như trong Authors và Categories collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def string_to_list(data):\n",
    "#     '''\n",
    "#     This function gets a list (authors, categories) from a string type\n",
    "    \n",
    "#     Args:\n",
    "#     data -- the input data needed to be converted\n",
    "    \n",
    "#     Returns:\n",
    "#     values -- the converted values (list type)\n",
    "#     '''\n",
    "#     # Process the string data to convert into list type\n",
    "#     values = data.strip(\"[]\").replace(\"'\", \"\")\n",
    "    \n",
    "#     # Remove the quotation marks\n",
    "#     values = values.replace('\"', '')\n",
    "    \n",
    "#     # Split the string to become a list\n",
    "#     values = values.split(\", \")\n",
    "    \n",
    "#     # Regex expression to remove invalid title\n",
    "#     pattern = re.compile(r'^[\\W\\d]+$')  # Remove strings containing only characters which are not \n",
    "\n",
    "#     # Remove invalid authors\n",
    "#     filtered_values = [value for value in values if not pattern.match(value)]\n",
    "    \n",
    "#     # Remove spaces at the beginning and ending of every element in the list\n",
    "#     values = list(value.strip() for value in filtered_values)\n",
    "    \n",
    "#     return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books = book_collection.find({'authors': {'$exists': True}})\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing...', position=0)\n",
    "\n",
    "# author_list = []\n",
    "\n",
    "# for book in books:\n",
    "#     new_authors = []\n",
    "#     authors = book['authors']\n",
    "    \n",
    "#     if (type(authors) == str):\n",
    "#         new_authors = string_to_list(authors)\n",
    "        \n",
    "#     elif (type(authors) == list):\n",
    "#         for author in authors:\n",
    "#             pattern = re.compile(r'^[\\W\\d]+$')  # Remove strings containing only characters which are not \n",
    "\n",
    "#             if author is not pattern.match(author):\n",
    "#                 continue\n",
    "            \n",
    "#             new_authors = authors\n",
    "    \n",
    "#     processed_authors = []\n",
    "#     for author in new_authors:\n",
    "#         author = author.lower()\n",
    "#         author = remove_special_characters(author)\n",
    "#         author = re.sub(r'\\s+', ' ', author)\n",
    "#         processed_authors.append(author)\n",
    "        \n",
    "#     author_list.append(({'_id': book['_id']}, {'$set': {'authors': processed_authors}}))\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "\n",
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for author in author_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             author[0],\n",
    "#             author[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books = book_collection.find({'categories': {'$exists': True}})\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing...', position=0)\n",
    "\n",
    "# category_list = []\n",
    "\n",
    "# for book in books:\n",
    "#     new_categories = []\n",
    "#     categories = book['categories']\n",
    "    \n",
    "#     if (type(categories) == str):\n",
    "#         new_categories = string_to_list(categories)\n",
    "        \n",
    "#     elif (type(categories) == list):\n",
    "#         for category in categories:\n",
    "#             pattern = re.compile(r'^[\\W\\d]+$')  # Remove strings containing only characters which are not \n",
    "\n",
    "#             if category is not pattern.match(category):\n",
    "#                 continue\n",
    "            \n",
    "#             new_categories = categories\n",
    "    \n",
    "#     processed_categories = []\n",
    "#     for category in new_categories:\n",
    "#         category = category.lower()\n",
    "#         category = remove_special_characters(category)\n",
    "#         category = re.sub(r'\\s+', ' ', category)\n",
    "#         processed_categories.append(category)\n",
    "        \n",
    "#     category_list.append(({'_id': book['_id']}, {'$set': {'categories': processed_categories}}))\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "\n",
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for category in category_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             category[0],\n",
    "#             category[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng Model:\n",
    "- SENTIMENT ANALYSIS đối với review/summary và review/text (RoBERTa-base model): biến chuỗi text thành dạng về một số (1: POS, 0: NEU, -1: NEG)\n",
    "- Tìm keyword của phần Description (BERT) (VD: Từ đoạn text \"Harry Potter this year fights Lord Voldemort and his Death Eaters\" -> [\"Harry Potter\", \"Lord Voldemort\", \"Death Eaters\"] -> \"Harry Potter Lord Voldemort Death Eaters\") -> Tính TF-IDF của phần Description giữa review input và toàn bộ review trong database -> Cho ra similarity với Description đầu vào\n",
    "- Tính TF-IDF các mục Authors, Title, Categories -> Cho ra similarity với Authors, Title, Categories đầu vào \n",
    "- Tính kNN (cosine similarity) giữa vector review vừa được thêm vào với tất cả review còn lại rồi trả về top 10 quyển có chỉ số sim cao nhất\n",
    "\n",
    "Cuối cùng là display nó lên cho người dùng xem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = []\n",
    "\n",
    "# for index, review_collection in enumerate(review_collections):\n",
    "#     print(f' --- Start converting review collection {index + 1} ---')\n",
    "#     df_review = pd.DataFrame(list(review_collection.find()))\n",
    "    \n",
    "#     dd_review = dd.from_pandas(df_review, npartitions=4)\n",
    "#     print(f' --- Done converting review collection {index + 1} ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = df.concat(df_reviews)\n",
    "# df_book = book_collection.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book_selected = df_book.loc[:, ['Title', 'authors', 'categories', 'description', 'review/count', 'review/mean_score', 'review/median_score', 'review/mode_score']]\n",
    "# df_reviews_selected = df_reviews.loc[:, ['Title', 'User_id', 'review/score', 'review/summary', 'review/text']]\n",
    "# merged_df = pd.merge(df_reviews_selected, df_book_selected, on='Title', how='inner')\n",
    "\n",
    "# merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv('D:\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file all.csv lên lại để đưa vào mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model_sentiment = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    # text = \"This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the place to find it -- there's only about 2 pages with text and everything else is photos.Bottom line: if you only want one book, the Six Foot One ... is probably a better choice, however, if you like Julie like I like Julie, you won't go wrong on this one either.\"\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model_sentiment(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "\n",
    "    highest_score = max(scores)\n",
    "    label = config.id2label[ranking[0]]\n",
    "    \n",
    "    if label == 'positive':\n",
    "        sentiment = 1\n",
    "    elif label == 'neutral':\n",
    "        sentiment = 0\n",
    "    elif label == 'negative':\n",
    "        sentiment = -1\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffile = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "\n",
    "# ffile['sentiment'] = ''\n",
    "\n",
    "# ffile.to_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "#                     #  chunksize=CHUNK_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# count = int(3500000 / CHUNK_SIZE)\n",
    "\n",
    "# sentiment_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=count, desc=f'Processing Sentiment Analysis...', position=0)\n",
    "\n",
    "# with open('sentiment.json', 'w') as json_file:\n",
    "    \n",
    "#     for chunk in df_all:\n",
    "        \n",
    "#         for index, row in chunk.iterrows():\n",
    "            \n",
    "#             review_text = html.unescape(row['review/text'])\n",
    "            \n",
    "#             title = row['Title']\n",
    "#             if title == '' or title == None:\n",
    "#                 continue\n",
    "            \n",
    "#             user_id = str(row['User_id'])\n",
    "#             if user_id == '' or user_id == None:\n",
    "#                 user_id = ''\n",
    "            \n",
    "#             if review_text == '' or review_text == None:\n",
    "#                 sentiment = 0\n",
    "#                 break\n",
    "\n",
    "#             sentiment_score, label = sentiment_analysis(review_text[:max_length])\n",
    "\n",
    "#             if label == 'positive':\n",
    "#                 sentiment = 1\n",
    "#             elif label == 'neutral':\n",
    "#                 sentiment = 0\n",
    "#             elif label == 'negative':\n",
    "#                 sentiment = -1\n",
    "                \n",
    "#             sentiment_json = {'Title': title, 'User_id': user_id, 'sentiment': sentiment}\n",
    "#             json.dump(sentiment_json, json_file)\n",
    "#             json_file.write(',\\n')\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 512\n",
    "\n",
    "# count = 0\n",
    "# sentiment_list = []\n",
    "\n",
    "# for review_collection in review_collections:\n",
    "\n",
    "#     reviews = review_collection.find()\n",
    "#     existed_num = review_collection.count_documents({'sentiment': {'$exists': True}})\n",
    "#     progress_bar = tqdm(total= (total_review - existed_num), desc=f'Processing Sentiment Analysis...', position=0)\n",
    "    \n",
    "#     for review in reviews.skip(existed_num):\n",
    "        \n",
    "#         review_text = html.unescape(str(review['review/text']))\n",
    "        \n",
    "#         if review_text == '' or review_text is None:\n",
    "#             sentiment = 0\n",
    "#         else:\n",
    "#             sentiment_score, label = sentiment_analysis(review_text[:max_length])\n",
    "#             if label == 'positive':\n",
    "#                 sentiment = 1\n",
    "#             elif label == 'neutral':\n",
    "#                 sentiment = 0\n",
    "#             elif label == 'negative':\n",
    "#                 sentiment = -1\n",
    "                \n",
    "#         sentiment_list.append(({'_id': review['_id']}, {'$set': {'sentiment': sentiment}}))\n",
    "        \n",
    "#         if count % 1000 == 0:\n",
    "#             update_requests = []\n",
    "\n",
    "#             for sentiment in sentiment_list:\n",
    "#                 update_requests.append(\n",
    "#                     UpdateOne(\n",
    "#                         sentiment[0],\n",
    "#                         sentiment[1]\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#             # Thực hiện bulk write\n",
    "#             result = review_collection_1.bulk_write(update_requests)\n",
    "#             sentiment_list = []\n",
    "            \n",
    "#         count += 1\n",
    "        \n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all User Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user_list = []\n",
    "\n",
    "# # for review_collection in review_collections:\n",
    "# #     progress_bar = tqdm(total=review_collection.count_documents({'User_id': {'$exists': True}}), desc='Getting users', position=0)\n",
    "\n",
    "# #     reviews = review_collection.find({'User_id': {'$exists': True}})\n",
    "\n",
    "# #     for review in reviews:\n",
    "# #         user_list.append(review['User_id'])\n",
    "# #         progress_bar.update(1)\n",
    "        \n",
    "# #     progress_bar.close()\n",
    "# user_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=users_collection.count_documents({'User_id': {'$exists': True}}), desc='Getting User id', position=0)\n",
    "\n",
    "# users = users_collection.find({'User_id': {'$exists': True}})\n",
    "\n",
    "# for user in users:\n",
    "#     user_list.append(user['User_id'])\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=book_collection.count_documents({'Title': {'$exists': True}}), desc='Getting titles', position=0)\n",
    "\n",
    "# books = book_collection.find({'Title': {'$exists': True}})\n",
    "\n",
    "# for book in books:\n",
    "#     title_list.append(book['Title'])\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_documents = [{'User_id': user_id} for user_id in user_list]\n",
    "\n",
    "# # Chèn các document mới vào collection\n",
    "# result = users_collection.insert_many(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.DataFrame(index=list(user_list), columns=title_list, dtype=int)\n",
    "# # print(df.memory_usage(index=True).sum())\n",
    "# # matrix = np.zeros((1000, 212404))\n",
    "# # memory_usage = matrix.nbytes\n",
    "\n",
    "# # # Chuyển đổi dung lượng bộ nhớ sang đơn vị GB\n",
    "# # memory_usage_gb = memory_usage / (1024**3)\n",
    "\n",
    "# selected_title = []\n",
    "# selected_books  = book_collection.find({'categories': {'$regex': 'fiction'}})\n",
    "# for book in selected_books:\n",
    "#     selected_title.append(book['Title'])\n",
    "# print(selected_title[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# progress_bar = tqdm(total=len(selected_title), desc='Getting user scores...', position=0)\n",
    "\n",
    "# # Tìm số sao theo từng title\n",
    "# for title in selected_title:\n",
    "#     title_and_users = [title]\n",
    "#     # Tìm trong 8 collection các review của quyển sách đó\n",
    "#     for review_collection in review_collections:\n",
    "#         found_reviews = review_collection.find({'Title': title})\n",
    "#         # Lấy thông tin của từng review đó\n",
    "#         for review in found_reviews:\n",
    "#             if 'User_id' in review:\n",
    "#                 user_score = {review['User_id']: review['review/score']}\n",
    "#                 title_and_users.append(user_score)\n",
    "#     user_item_list.append(title_and_users)\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# data = df_all\n",
    "# y = data['review/score']\n",
    "# X = data.drop('review/score', axis=1)\n",
    "# print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Tokenize các review\n",
    "# data['tokenized_title'] = data['Title'].apply(nltk.word_tokenize)\n",
    "\n",
    "# # Huấn luyện mô hình Word2Vec\n",
    "# model = Word2Vec(data['tokenized_title'], min_count=1)\n",
    "\n",
    "# # Lấy vector đại diện cho từ 'book'\n",
    "# vector = model.wv['book']\n",
    "\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # scaler = StandardScaler()\n",
    "# # X_train = scaler.fit_transform(X_train)\n",
    "# # X_test = scaler.transform(X_test)\n",
    "\n",
    "# # # Xây dựng mô hình\n",
    "# # model = Sequential()\n",
    "# # model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # model.add(Dense(64, activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# # model.add(Dense(1))\n",
    "\n",
    "# # # Huấn luyện mô hình\n",
    "# # model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# # model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3404614, 13)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "                    #  chunksize=CHUNK_SIZE,)\n",
    "                    \n",
    "df_all.drop_duplicates(inplace = True)\n",
    "df_all.drop(df_all.columns[[0,1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2233287, 13)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.dropna(subset=['User_id', 'review/mean_score'], inplace=True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2233287, 13)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df_all\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "users = data['User_id'].unique()\n",
    "items = data['Title'].unique()\n",
    "\n",
    "# Tạo từ điển ánh xạ giữa user hoặc item và chỉ mục của chúng trong mảng NumPy\n",
    "user_dict = {user: i for i, user in enumerate(users)}\n",
    "item_dict = {item: i for i, item in enumerate(items)}\n",
    "average_rating_dict = {row['Title']: row['review/mean_score'] for i, row in data.iterrows()}\n",
    "\n",
    "# Xây dựng ma trận người dùng-sản phẩm\n",
    "user_item_matrix = csr_matrix((len(users), len(items)), dtype=float)\n",
    "\n",
    "# Điền giá trị đánh giá vào ma trận\n",
    "for _, row in data.iterrows():\n",
    "    user_index = user_dict.get(row['User_id'])\n",
    "    item_index = item_dict.get(row['Title'])\n",
    "    user_item_matrix[user_index, item_index] = row['review/score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79777\n",
      "37720\n",
      "37720\n"
     ]
    }
   ],
   "source": [
    "print(len(user_dict))\n",
    "print(len(item_dict))\n",
    "print(len(average_rating_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t5.0\n",
      "  (1, 1)\t4.0\n",
      "  (2, 2)\t4.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t5.0\n",
      "  (4, 14161)\t4.0\n",
      "  (4, 17233)\t3.0\n",
      "  (4, 35724)\t3.0\n",
      "  (5, 5)\t5.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t5.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t5.0\n"
     ]
    }
   ],
   "source": [
    "print(user_item_matrix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hereward the wake last of the english: 5.0\n",
      "the giver: 4.2734170591313445\n",
      "catch 22: 4.391074856046065\n",
      "48 laws of power: 4.149739583333333\n",
      "islamic history a framework for inquiry: 4.0\n",
      "needlepoint by design variations on chinese themes: 5.0\n",
      "first aid for the usmle step 1 2000 a student to student guide: 4.416666666666667\n",
      "visitor: 3.668269230769231\n",
      "can black mothers raise our sons: 4.0\n",
      "fellowship of the ring 2nd edition: 4.4539282250242485\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "# In ra cặp key-value đầu tiên của từ điển\n",
    "for key, value in average_rating_dict.items():\n",
    "    if count == 10:\n",
    "        break\n",
    "    print(f\"{key}: {value}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UBCF:\\\n",
    "Tính toán giá trị rating dự đoán của toàn bộ sách đối với một user target.\\\n",
    "Tìm top k=10 sách có giá trị rating cao nhất và đề xuất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 4.2734170591313445, 4.391074856046065, 4.149739583333333, 4.0, 5.0, 4.416666666666667, 3.668269230769231, 4.0, 4.4539282250242485]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "r_mean = list(average_rating_dict.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giá trị sentiment:\n",
    "- Positive: 1\n",
    "- Neutral: 0\n",
    "- Negative: -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>User_id</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>review/count</th>\n",
       "      <th>review/mean_score</th>\n",
       "      <th>review/median_score</th>\n",
       "      <th>review/mode_score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3251645</th>\n",
       "      <td>little women or meg jo beth and amy parts i an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Two men have now read Little Women!!!</td>\n",
       "      <td>In the senior class of the &amp;quot;Great Books&amp;q...</td>\n",
       "      <td>['louisa may alcott']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832.0</td>\n",
       "      <td>4.447115</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425673</th>\n",
       "      <td>dragonwyck</td>\n",
       "      <td>A1Q54C284A04ZP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dragonwydk</td>\n",
       "      <td>This is one of the most beautifully writen sto...</td>\n",
       "      <td>['anya seton']</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>el libro de anya seton que inspir la obra maes...</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4.453782</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415256</th>\n",
       "      <td>speaker for the dead</td>\n",
       "      <td>A3UCWQQR5AL944</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Breath-taking and stunning</td>\n",
       "      <td>I found this book amazing. It is not as fast-p...</td>\n",
       "      <td>['orson scott card']</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>in the aftermath of his terrible war ender wig...</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>4.294960</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647430</th>\n",
       "      <td>the book of mormon another testament of jesus ...</td>\n",
       "      <td>ACTR3SZ8FJFQZ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The book that changed my life</td>\n",
       "      <td>This book is ancient scripture of a fallen peo...</td>\n",
       "      <td>['joseph smith jr']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>955.0</td>\n",
       "      <td>4.015707</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841794</th>\n",
       "      <td>moby dick or the white whale</td>\n",
       "      <td>A35N8ZH1AEGM4B</td>\n",
       "      <td>5.0</td>\n",
       "      <td>excellent</td>\n",
       "      <td>Good product, good service, good value. Everyt...</td>\n",
       "      <td>['herman melville']</td>\n",
       "      <td>['fiction']</td>\n",
       "      <td>moby dick is a novel by american writer herman...</td>\n",
       "      <td>551.0</td>\n",
       "      <td>4.038113</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Title         User_id  \\\n",
       "3251645  little women or meg jo beth and amy parts i an...             NaN   \n",
       "1425673                                         dragonwyck  A1Q54C284A04ZP   \n",
       "2415256                               speaker for the dead  A3UCWQQR5AL944   \n",
       "1647430  the book of mormon another testament of jesus ...   ACTR3SZ8FJFQZ   \n",
       "2841794                       moby dick or the white whale  A35N8ZH1AEGM4B   \n",
       "\n",
       "         review/score                         review/summary  \\\n",
       "3251645           5.0  Two men have now read Little Women!!!   \n",
       "1425673           5.0                             Dragonwydk   \n",
       "2415256           5.0             Breath-taking and stunning   \n",
       "1647430           5.0          The book that changed my life   \n",
       "2841794           5.0                              excellent   \n",
       "\n",
       "                                               review/text  \\\n",
       "3251645  In the senior class of the &quot;Great Books&q...   \n",
       "1425673  This is one of the most beautifully writen sto...   \n",
       "2415256  I found this book amazing. It is not as fast-p...   \n",
       "1647430  This book is ancient scripture of a fallen peo...   \n",
       "2841794  Good product, good service, good value. Everyt...   \n",
       "\n",
       "                       authors   categories  \\\n",
       "3251645  ['louisa may alcott']          NaN   \n",
       "1425673         ['anya seton']  ['fiction']   \n",
       "2415256   ['orson scott card']  ['fiction']   \n",
       "1647430    ['joseph smith jr']          NaN   \n",
       "2841794    ['herman melville']  ['fiction']   \n",
       "\n",
       "                                               description  review/count  \\\n",
       "3251645                                                NaN         832.0   \n",
       "1425673  el libro de anya seton que inspir la obra maes...         119.0   \n",
       "2415256  in the aftermath of his terrible war ender wig...        1746.0   \n",
       "1647430                                                NaN         955.0   \n",
       "2841794  moby dick is a novel by american writer herman...         551.0   \n",
       "\n",
       "         review/mean_score  review/median_score  review/mode_score  sentiment  \\\n",
       "3251645           4.447115                  5.0                5.0        NaN   \n",
       "1425673           4.453782                  5.0                5.0        NaN   \n",
       "2415256           4.294960                  5.0                5.0        NaN   \n",
       "1647430           4.015707                  5.0                5.0        NaN   \n",
       "2841794           4.038113                  5.0                5.0        NaN   \n",
       "\n",
       "         Sentiment  \n",
       "3251645          1  \n",
       "1425673          1  \n",
       "2415256          1  \n",
       "1647430          1  \n",
       "2841794          1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['Sentiment'] = data['review/text'].apply(lambda review: sentiment_analysis(review[:512]))\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['User_id', 'review/summary', 'review/text', 'authors', 'categories', 'description', 'review/count', 'review/mean_score', 'review/median_score', 'review/mode_score', 'sentiment'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Chuẩn bị dữ liệu đầu vào\n",
    "titles = data['Title']\n",
    "\n",
    "# Huấn luyện mô hình Word2Vec\n",
    "model_w2v = Word2Vec(sentences=titles.str.lower().str.split(), vector_size=1, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Áp dụng Word2Vec vào mỗi tiêu đề trong dataframe\n",
    "data['Title_Embedding'] = data['Title'].apply(lambda title: model_w2v.wv[str(title).lower().split()])\n",
    "data['Title_Embedding'] = data['Title_Embedding'].apply(lambda embedding: np.array(embedding).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>review/score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Title_Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3251645</th>\n",
       "      <td>little women or meg jo beth and amy parts i an...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.9196033, 6.1022587, 7.678004, 5.961695, 6.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425673</th>\n",
       "      <td>dragonwyck</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5059984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415256</th>\n",
       "      <td>speaker for the dead</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.0376395, 8.393242, 8.029349, 0.50631446]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647430</th>\n",
       "      <td>the book of mormon another testament of jesus ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[8.029349, 6.3486176, 7.8763466, -0.10440799, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841794</th>\n",
       "      <td>moby dick or the white whale</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.3649082, 1.996075, 7.678004, 8.029349, 0.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>the scarlet letter a romance</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[8.029349, 0.8751762, 2.7037842, 7.750972, 2.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2339122</th>\n",
       "      <td>cathedrals of the flesh</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.72870094, 7.8763466, 8.029349, 0.9555524]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473604</th>\n",
       "      <td>phantom</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6034404]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694556</th>\n",
       "      <td>japanese ii comprehensive learn to speak and u...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6582789, 1.8055787, 0.3369542, 0.57875305, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188593</th>\n",
       "      <td>to lose a battle france 1940</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7.4187126, 1.0708256, 7.750972, 0.6252861, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662545</th>\n",
       "      <td>a room of ones own</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[7.750972, 0.42624408, 7.8763466, -0.24561355,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984415</th>\n",
       "      <td>the adventures of huckleberry finn</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[8.029349, 3.7883382, 7.8763466, -0.361872, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132962</th>\n",
       "      <td>starring hillary picture books</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.53937966, 0.3166168, 1.8653916, 3.6845767]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916336</th>\n",
       "      <td>a single shard</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[7.750972, 1.0195414, 1.0057936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956911</th>\n",
       "      <td>dynamic judograppling techniques</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.0005776, 0.22725803, 2.4934914]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255955</th>\n",
       "      <td>the land of promise</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[8.029349, 1.9556392, 7.8763466, -0.3350966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702297</th>\n",
       "      <td>zen and the art of motorcycle maintenance</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.59957486, 9.648727, 8.029349, 3.051251, 7.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887014</th>\n",
       "      <td>alices adventures in wonderland</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.2519057, 3.7883382, 7.3333592, 1.9913155]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991589</th>\n",
       "      <td>as time goes by</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[2.803098, 3.1441813, -0.53475475, 5.705396]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006583</th>\n",
       "      <td>bloodstone</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5206115]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Title  review/score  \\\n",
       "3251645  little women or meg jo beth and amy parts i an...           5.0   \n",
       "1425673                                         dragonwyck           5.0   \n",
       "2415256                               speaker for the dead           5.0   \n",
       "1647430  the book of mormon another testament of jesus ...           5.0   \n",
       "2841794                       moby dick or the white whale           5.0   \n",
       "16996                         the scarlet letter a romance           5.0   \n",
       "2339122                            cathedrals of the flesh           5.0   \n",
       "2473604                                            phantom           5.0   \n",
       "2694556  japanese ii comprehensive learn to speak and u...           5.0   \n",
       "1188593                       to lose a battle france 1940           5.0   \n",
       "1662545                                 a room of ones own           1.0   \n",
       "1984415                 the adventures of huckleberry finn           5.0   \n",
       "3132962                     starring hillary picture books           4.0   \n",
       "1916336                                     a single shard           5.0   \n",
       "2956911                   dynamic judograppling techniques           5.0   \n",
       "3255955                                the land of promise           5.0   \n",
       "702297           zen and the art of motorcycle maintenance           1.0   \n",
       "887014                     alices adventures in wonderland           3.0   \n",
       "1991589                                    as time goes by           4.0   \n",
       "3006583                                         bloodstone           4.0   \n",
       "\n",
       "         Sentiment                                    Title_Embedding  \n",
       "3251645          1  [4.9196033, 6.1022587, 7.678004, 5.961695, 6.6...  \n",
       "1425673          1                                        [0.5059984]  \n",
       "2415256          1        [1.0376395, 8.393242, 8.029349, 0.50631446]  \n",
       "1647430          1  [8.029349, 6.3486176, 7.8763466, -0.10440799, ...  \n",
       "2841794          1  [2.3649082, 1.996075, 7.678004, 8.029349, 0.92...  \n",
       "16996            0  [8.029349, 0.8751762, 2.7037842, 7.750972, 2.0...  \n",
       "2339122          1       [0.72870094, 7.8763466, 8.029349, 0.9555524]  \n",
       "2473604          1                                        [0.6034404]  \n",
       "2694556          1  [0.6582789, 1.8055787, 0.3369542, 0.57875305, ...  \n",
       "1188593          0  [7.4187126, 1.0708256, 7.750972, 0.6252861, 0....  \n",
       "1662545         -1  [7.750972, 0.42624408, 7.8763466, -0.24561355,...  \n",
       "1984415          1  [8.029349, 3.7883382, 7.8763466, -0.361872, 1....  \n",
       "3132962          0     [-0.53937966, 0.3166168, 1.8653916, 3.6845767]  \n",
       "1916336          1                   [7.750972, 1.0195414, 1.0057936]  \n",
       "2956911          1                 [1.0005776, 0.22725803, 2.4934914]  \n",
       "3255955          1       [8.029349, 1.9556392, 7.8763466, -0.3350966]  \n",
       "702297          -1  [0.59957486, 9.648727, 8.029349, 3.051251, 7.8...  \n",
       "887014          -1       [0.2519057, 3.7883382, 7.3333592, 1.9913155]  \n",
       "1991589          1       [2.803098, 3.1441813, -0.53475475, 5.705396]  \n",
       "3006583          0                                        [0.5206115]  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/score</th>\n",
       "      <th>compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Title_Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1875764</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.7112393, 0.90833944, 1.2833309]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117820</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.7112393, 4.9162383, 1.5926683, 1.0353863, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372356</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>1</td>\n",
       "      <td>[6.694695, 2.257814, 5.39001, 5.7112393, 4.035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881123</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.7070</td>\n",
       "      <td>-1</td>\n",
       "      <td>[3.965957, 6.4527774, 6.134444, 4.11004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254015</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.9848</td>\n",
       "      <td>-1</td>\n",
       "      <td>[2.9734485, 2.408457, 6.277846, 2.1698585, 1.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         review/score  compound  Sentiment  \\\n",
       "1875764           5.0    0.9950          1   \n",
       "2117820           5.0    0.8519          1   \n",
       "372356            1.0    0.8625          1   \n",
       "881123            3.0   -0.7070         -1   \n",
       "1254015           4.0   -0.9848         -1   \n",
       "\n",
       "                                           Title_Embedding  \n",
       "1875764                 [5.7112393, 0.90833944, 1.2833309]  \n",
       "2117820  [5.7112393, 4.9162383, 1.5926683, 1.0353863, 0...  \n",
       "372356   [6.694695, 2.257814, 5.39001, 5.7112393, 4.035...  \n",
       "881123            [3.965957, 6.4527774, 6.134444, 4.11004]  \n",
       "1254015  [2.9734485, 2.408457, 6.277846, 2.1698585, 1.5...  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.drop(['Title'], axis=1, inplace=True)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3251645    5.474820\n",
       "1425673    0.505998\n",
       "2415256    4.491637\n",
       "1647430    4.110224\n",
       "2841794    3.866624\n",
       "Name: Title_Embedding_Mean, dtype: float32"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Title_Embedding_Sum'] = data['Title_Embedding'].apply(lambda embedding: np.sum(embedding))\n",
    "data['Title_Embedding_Sum'].head()\n",
    "data['Title_Embedding_Mean'] = data['Title_Embedding'].apply(lambda embedding: np.mean(embedding))\n",
    "data['Title_Embedding_Mean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 17.6564 - accuracy: 0.0787 - val_loss: 17.5869 - val_accuracy: 0.0806\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 17.6407 - accuracy: 0.0825 - val_loss: 17.5853 - val_accuracy: 0.1369\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 17.6344 - accuracy: 0.0728 - val_loss: 17.5851 - val_accuracy: 0.0319\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 17.6327 - accuracy: 0.0698 - val_loss: 17.5850 - val_accuracy: 0.0494\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 17.6322 - accuracy: 0.0670 - val_loss: 17.5850 - val_accuracy: 0.0456\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 17.6320 - accuracy: 0.0755 - val_loss: 17.5850 - val_accuracy: 0.0625\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 17.6319 - accuracy: 0.0830 - val_loss: 17.5850 - val_accuracy: 0.1163\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 17.6319 - accuracy: 0.0822 - val_loss: 17.5850 - val_accuracy: 0.0950\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 17.6319 - accuracy: 0.0823 - val_loss: 17.5850 - val_accuracy: 0.1044\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 17.6319 - accuracy: 0.0889 - val_loss: 17.5850 - val_accuracy: 0.0850\n",
      "63/63 [==============================] - 0s 858us/step - loss: 17.6898 - accuracy: 0.0810\n",
      "Mean Squared Error: [17.689800262451172, 0.08100000023841858]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chia dữ liệu thành features (X) và target (y)\n",
    "X = data[['Sentiment', 'Title_Embedding_Sum', 'Title_Embedding_Mean']]\n",
    "y = data['review/score']\n",
    "\n",
    "# # Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Khởi tạo mô hình mạng nơ-ron\n",
    "model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile mô hình với loss function là 'mean_squared_error' và optimizer là 'adam'\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Huấn luyện mô hình trên tập huấn luyện\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Đánh giá hiệu suất của mô hình trên tập kiểm tra\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Khởi tạo mô hình KNN với k = 3 (có thể điều chỉnh giá trị k tùy ý)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Huấn luyện mô hình trên tập huấn luyện\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán trên tập kiểm tra\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Đánh giá hiệu suất của mô hình bằng độ chính xác\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 nearest neighbors for book 1:\n",
      "Neighbor 1: gods little acre (review score: 2.0)\n",
      "Neighbor 2: recipes for longer life (review score: 5.0)\n",
      "Neighbor 3: the hobbit (review score: 5.0)\n",
      "Neighbor 4: serpents walk (review score: 5.0)\n",
      "Neighbor 5: dharma beads making and using your own buddhist malas (review score: 5.0)\n",
      "Neighbor 6: child behavior (review score: 4.0)\n",
      "Neighbor 7: why revival tarries (review score: 4.0)\n",
      "Neighbor 8: crucible the (review score: 5.0)\n",
      "Neighbor 9: small dolls of the 40s and 50s identification and value guide (review score: 5.0)\n",
      "Neighbor 10: the golden compass (review score: 4.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_title = 'harry potter and the chamber of secrets'\n",
    "\n",
    "new_title_embedding = model_w2v.wv[str(new_title).lower().split()]\n",
    "new_title_embedding = np.array(new_title_embedding).flatten()\n",
    "new_title_embedding_sum = np.sum(new_title_embedding)\n",
    "new_title_embedding_mean = np.mean(new_title_embedding)\n",
    "\n",
    "new_review = 'This book is great'\n",
    "new_sentiment = sentiment_analysis(new_review)\n",
    "\n",
    "distances, indices = knn_model.kneighbors([[new_sentiment, new_title_embedding_sum, new_title_embedding_mean]])\n",
    "\n",
    "for i, book_index in enumerate(indices):\n",
    "    print(f\"Top 10 nearest neighbors for book {i + 1}:\")\n",
    "    for j, neighbor_index in enumerate(book_index):\n",
    "        neighbor_title = data.iloc[neighbor_index]['Title']\n",
    "        neighbor_score = y_train.iloc[neighbor_index]\n",
    "        print(f\"Neighbor {j + 1}: {neighbor_title} (review score: {neighbor_score})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
