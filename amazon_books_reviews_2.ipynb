{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import getpass\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import chain\n",
    "import datetime\n",
    "from fractions import Fraction\n",
    "from collections import Counter\n",
    "from pymongo import UpdateOne\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "import dask.dataframe as dd\n",
    "import html\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath_book = \"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\books_data.csv\"\n",
    "# filepath = \"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\Books_rating.csv\"\n",
    "\n",
    "# df_book = pd.read_csv(filepath_book)\n",
    "# df_review = pd.read_csv(filepath)\n",
    "\n",
    "password = 'Ngthiennhan2002.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_index = 0\n",
    "# chunk_size = 375000\n",
    "\n",
    "# for i in range(1, 9):\n",
    "#     end_index = start_index + chunk_size\n",
    "#     df_chunk = df_review.iloc[start_index:end_index]\n",
    "#     file_name = f\"D:\\\\University\\\\Môn học\\\\Ứng dụng Big Data\\\\Project\\\\Datasets\\\\Amazon Books Reviews\\\\Books_rating_{i}_375k.csv\"\n",
    "#     df_chunk.to_csv(file_name, index=False)\n",
    "#     start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\cryptography\\x509\\base.py:594: CryptographyDeprecationWarning: Parsed a negative serial number, which is disallowed by RFC 5280.\n",
      "  return rust_x509.load_der_x509_certificate(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "375000\n",
      "212404\n"
     ]
    }
   ],
   "source": [
    "API_1 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.mhlvibl.mongodb.net/'\n",
    "# API_1 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.yl3o8ez.mongodb.net/'\n",
    "API_2 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.bzvhw41.mongodb.net/'\n",
    "API_3 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.jrdv2e2.mongodb.net/'\n",
    "API_4 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.tq84xea.mongodb.net/'\n",
    "API_5 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.i2p6hb8.mongodb.net/'\n",
    "API_6 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.ppuo86b.mongodb.net/'\n",
    "API_7 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.xjtimov.mongodb.net/'\n",
    "API_8 = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.5ihto1h.mongodb.net/'\n",
    "API_items = f'mongodb+srv://ngthiennhan2002:{password}@cluster0.2qutpiu.mongodb.net/'\n",
    "\n",
    "try:\n",
    "    # Create two MongoDB clients using MongoClient with two APIs\n",
    "    client_1 = MongoClient(API_1)\n",
    "    client_2 = MongoClient(API_2)\n",
    "    client_3 = MongoClient(API_3)\n",
    "    client_4 = MongoClient(API_4)\n",
    "    client_5 = MongoClient(API_5)\n",
    "    client_6 = MongoClient(API_6)\n",
    "    client_7 = MongoClient(API_7)\n",
    "    client_8 = MongoClient(API_8)\n",
    "    client_items = MongoClient(API_items)\n",
    "    \n",
    "    # Variables to save names of databases and collections\n",
    "    database_name = 'db'\n",
    "    review_collection_name = 'Reviews'\n",
    "    book_collection_name = 'Books'\n",
    "    author_collection_name = 'Authors'\n",
    "    categories_collection_name = 'Categories'\n",
    "    temp_collection_name = 'Temp'\n",
    "    user_collection_name = 'Users'\n",
    "\n",
    "    db_1 = client_1[database_name]\n",
    "    review_collection_1 = db_1[review_collection_name]\n",
    "\n",
    "    db_2 = client_2[database_name]\n",
    "    review_collection_2 = db_2[review_collection_name]\n",
    "    \n",
    "    db_3 = client_3[database_name]\n",
    "    review_collection_3 = db_3[review_collection_name]\n",
    "\n",
    "    db_4 = client_4[database_name]\n",
    "    review_collection_4 = db_4[review_collection_name]\n",
    "    \n",
    "    db_5 = client_5[database_name]\n",
    "    review_collection_5 = db_5[review_collection_name]\n",
    "\n",
    "    db_6 = client_6[database_name]\n",
    "    review_collection_6 = db_6[review_collection_name]\n",
    "    \n",
    "    db_7 = client_7[database_name]\n",
    "    review_collection_7 = db_7[review_collection_name]\n",
    "\n",
    "    db_8 = client_8[database_name]\n",
    "    review_collection_8 = db_8[review_collection_name]\n",
    "    \n",
    "    db_items = client_items[database_name]\n",
    "    book_collection = db_items[book_collection_name]\n",
    "    author_collection = db_items[author_collection_name]\n",
    "    categories_collection = db_items[categories_collection_name]\n",
    "    temp_collection = db_items[temp_collection_name]\n",
    "    users_collection = db_items[user_collection_name]\n",
    "except:\n",
    "    print(\"Incorrect password or cannot connect to MongoDB at this time\")\n",
    "    \n",
    "# Test if the documents are read successfully\n",
    "print(review_collection_1.count_documents({}))\n",
    "print(review_collection_2.count_documents({}))\n",
    "print(review_collection_3.count_documents({}))\n",
    "print(review_collection_4.count_documents({}))\n",
    "print(review_collection_5.count_documents({}))\n",
    "print(review_collection_6.count_documents({}))\n",
    "print(review_collection_7.count_documents({}))\n",
    "print(review_collection_8.count_documents({}))\n",
    "print(book_collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lengths of collections\n",
    "total_book = book_collection.count_documents({})\n",
    "total_review = 375000\n",
    "\n",
    "# Convert book collection into pandas DataFrame (with progress bar)\n",
    "# with tqdm(total=total_book, desc='Converting book collection') as pbar:\n",
    "#     df_book = pd.DataFrame(list(book_collection.find()))\n",
    "#     pbar.update(len(df_book))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 1') as pbar:\n",
    "#     df_review_1 = pd.DataFrame(list(review_collection_1.find()))\n",
    "#     pbar.update(len(df_review_1))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 2') as pbar:\n",
    "#     df_review_2 = pd.DataFrame(list(review_collection_2.find()))\n",
    "#     pbar.update(len(df_review_2))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 3') as pbar:\n",
    "#     df_review_3 = pd.DataFrame(list(review_collection_3.find()))\n",
    "#     pbar.update(len(df_review_3))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 4') as pbar:\n",
    "#     df_review_4 = pd.DataFrame(list(review_collection_4.find()))\n",
    "#     pbar.update(len(df_review_4))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 5') as pbar:\n",
    "#     df_review_5 = pd.DataFrame(list(review_collection_5.find()))\n",
    "#     pbar.update(len(df_review_5))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 6') as pbar:\n",
    "#     df_review_6 = pd.DataFrame(list(review_collection_6.find()))\n",
    "#     pbar.update(len(df_review_6))\n",
    "#     pbar.close()\n",
    "    \n",
    "# with tqdm(total=total_review, desc='Converting review collection 7') as pbar:\n",
    "#     df_review_7 = pd.DataFrame(list(review_collection_7.find()))\n",
    "#     pbar.update(len(df_review_7))\n",
    "#     pbar.close()\n",
    "\n",
    "# with tqdm(total=total_review, desc='Converting review collection 8') as pbar:\n",
    "#     df_review_8 = pd.DataFrame(list(review_collection_8.find()))\n",
    "#     pbar.update(len(df_review_8))\n",
    "#     pbar.close()\n",
    "\n",
    "# print(\"--- Finished converting to DataFrame ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two df_review_1 and df_review_2\n",
    "# with tqdm(total=total_review, desc='Combining 2 review collections') as pbar:\n",
    "#     df_review = pd.concat([df_review_1, df_review_2, df_review_3, df_review_4, df_review_5, df_review_6, df_review_7, df_review_8])\n",
    "#     pbar.update(len(df_review))\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Title\\\n",
    "Missing values: 1 book thiếu Title -> điền thủ công bằng cách vào Link\\\n",
    "Noises: Chuyển chuỗi về kí tự thường và loại bỏ các kí tự đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count = 0\n",
    "\n",
    "# # for temp in temp_collection.find():\n",
    "# #     if temp['field'] == 'book_title':\n",
    "# #         temp_id = temp['_id']\n",
    "# #         count = int(temp['value'])\n",
    "# #         break\n",
    "\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Title', position=0)\n",
    "\n",
    "# title_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     title = book['Title']\n",
    "#     if type(title) is not str: \n",
    "#         title = str(title)\n",
    "#     title = title.lower()\n",
    "#     title = remove_special_characters(title)\n",
    "#     title = re.sub(r'\\s+', ' ', title)\n",
    "#     title_list.append(({'_id': book['_id']}, {'$set': {'Title': title}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for title in title_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             title[0],\n",
    "#             title[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Description\n",
    "- Missing values: Bỏ qua\n",
    "- Noises: Chuyển chuỗi về chữ thường và loại bỏ các kí tự đặc biệt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Description', position=0)\n",
    "\n",
    "# description_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     if 'description' in book:\n",
    "#         description = str(book['description'])\n",
    "#         description = description.lower()\n",
    "#         description = remove_special_characters(description)\n",
    "#         description = re.sub(r'\\s+', ' ', description)\n",
    "#         description_list.append(({'_id': book['_id']}, {'$set': {'description': description}}))\n",
    "#     # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for description in description_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             description[0],\n",
    "#             description[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Authors và Categories\n",
    "- Missing values: Bỏ qua\n",
    "- Tạo danh sách chuỗi các tác giả\n",
    "- Noises: Chuyển chuỗi về chữ thường, loại bỏ các kí tự đặc biệt và xử lý các biến thể tên tác giả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = []\n",
    "# categories = []\n",
    "\n",
    "# for author in author_collection.find():\n",
    "#     author_id = author['_id']\n",
    "#     authors.append(({'_id': author_id}, {'$set': {'Value': author['Value']}}))\n",
    "    \n",
    "# for category in categories_collection.find():\n",
    "#     category_id = category['_id']\n",
    "#     categories.append(({'_id': category_id}, {'$set': {'Value': category['Value']}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=author_collection.count_documents({}), desc='Preprocessing Authors\\'s collection', position=0)\n",
    "\n",
    "# author_list = []\n",
    "\n",
    "# for author in author_collection.find():\n",
    "#     author_name = author['Value']\n",
    "#     if author_name:\n",
    "#         author_name = author_name.lower()\n",
    "#         author_name = remove_special_characters(author_name)\n",
    "#         author_name = re.sub(r'\\s+', ' ', author_name)\n",
    "#         author_list.append(({'_id': author['_id']}, {'$set': {'Value': author_name}}))\n",
    "#     # author_list.append({'_id': temp_id}, {'$set': {'Value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for author in author_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             {'_id': author[0]['_id']},\n",
    "#             {'$set': {'Value': author[1]['$set']['Value']}},\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = author_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=categories_collection.count_documents({}), desc='Preprocessing Categories\\'s collection', position=0)\n",
    "\n",
    "# category_list = []\n",
    "\n",
    "# for category in categories_collection.find():\n",
    "#     category_name = category['Value']\n",
    "#     category_name = category_name.lower()\n",
    "#     category_name = remove_special_characters(category_name)\n",
    "#     category_name = re.sub(r'\\s+', ' ', category_name)\n",
    "#     category_list.append(({'_id': category['_id']}, {'$set': {'Value': category_name}}))\n",
    "#     # temp_collection.update_one({'_id': temp_id}, {'$set': {'Value': count}})\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for category in category_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             category[0],\n",
    "#             category[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = categories_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Publisher\n",
    "- Missing values: Bỏ qua\n",
    "- Noises: Loại bỏ kí tự đặc biệt, chuyển từ chữ hoa sang chữ thường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing book collection\\'s Publisher', position=0)\n",
    "\n",
    "# publisher_list = []\n",
    "\n",
    "# for book in book_collection.find():\n",
    "#     if 'publisher' in book:\n",
    "#         publisher = book['publisher']\n",
    "#         # print(publisher)\n",
    "#         if type(publisher) is int:\n",
    "#             publisher = str(publisher)\n",
    "#         publisher = publisher.lower()\n",
    "#         publisher = remove_special_characters(publisher)\n",
    "#         publisher = re.sub(r'\\s+', ' ', publisher)\n",
    "#         publisher_list.append(({'_id': book['_id']}, {'$set': {'publisher': publisher}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "# for publisher in publisher_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             publisher[0],\n",
    "#             publisher[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = book_collection.bulk_write(update_requests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường publishedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# def convert_to_datetime(date_str):\n",
    "#     formats = ['%Y', '%Y-%m', '%m-%Y', '%Y-%m-%d', '%m-%d-%Y', '%Y/%m/%d', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M', '%m/%d/%Y', '%d/%m/%Y']\n",
    "#     if isinstance(date_str, datetime):\n",
    "#         return date_str\n",
    "    \n",
    "#     if isinstance(date_str, int):\n",
    "#         date_str = str(date_str)\n",
    "        \n",
    "#     for fmt in formats:\n",
    "#         try:\n",
    "#             date_obj = datetime.strptime(date_str, fmt)\n",
    "#             if fmt.count('%') == 1:\n",
    "#                 return date_obj.strftime('%Y')\n",
    "#             elif fmt.count('%') == 2:\n",
    "#                 return date_obj.strftime('%Y-%m')\n",
    "#             else:\n",
    "#                 return date_obj.strftime('%Y-%m-%d')\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "#     return None\n",
    "    \n",
    "# count = 0\n",
    "\n",
    "# a = []\n",
    "\n",
    "# progress_bar = tqdm(total=total_book-count, desc='Preprocessing book collection\\'s Published Date', position=0)\n",
    "\n",
    "# for book in book_collection.find().skip(count):\n",
    "#     if 'publishedDate' in book:\n",
    "#         publishedDate = book['publishedDate']\n",
    "#         if publishedDate:\n",
    "#             try:\n",
    "#                 publishedDate = convert_to_datetime(publishedDate)\n",
    "#                 # book_collection.update_one({'_id': book['_id']}, {'$set': {'publishedDate': publishedDate}})\n",
    "#                 a.append(({'_id': book['_id']}, {'$set': {'publishedDate': publishedDate}}))\n",
    "#             except ValueError:\n",
    "#                 print(\"Error\")\n",
    "#         count += 1\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "# # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': 0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# published_date_list = a\n",
    "\n",
    "# for published_date in published_date_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             published_date[0],\n",
    "#             published_date[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # # Thực hiện bulk write\n",
    "# result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Title (của Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_collections = [review_collection_1,\n",
    "                     review_collection_2,\n",
    "                     review_collection_3,\n",
    "                     review_collection_4,\n",
    "                     review_collection_5,\n",
    "                     review_collection_6,\n",
    "                     review_collection_7,\n",
    "                     review_collection_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, review_collection in enumerate(review_collections):\n",
    "    \n",
    "#     print(f'--- Preprocessing title of review collection {index + 1} ---')\n",
    "#     title_list = []\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review collection\\'s Title', position=0)\n",
    "    \n",
    "#     for index_2, review in enumerate(review_collection.find()):\n",
    "#         if 'Title' in review:\n",
    "#             title = review['Title']\n",
    "#             if type(title) is not str: \n",
    "#                 title = str(title)\n",
    "#             # title = title.lower()\n",
    "#             # title = remove_special_characters(title)\n",
    "#             title = re.sub(r'\\s+', ' ', title)\n",
    "#             title_list.append(({'_id': review['_id']}, {'$set': {'Title': title}}))\n",
    "#         # temp_collection.update_one({'_id': temp_id}, {'$set': {'value': count}})\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()\n",
    "    \n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     update_requests = []\n",
    "    \n",
    "#     for title in title_list:\n",
    "#         update_requests.append(\n",
    "#             UpdateOne(\n",
    "#                 title[0],\n",
    "#                 title[1],\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     print(update_requests[:5])\n",
    "\n",
    "#     # # Thực hiện bulk write\n",
    "#     # result = review_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường review/count và trường review/average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sử dụng defaultdict để tổ chức dữ liệu theo 'Title'\n",
    "# title_data = defaultdict(list)\n",
    "\n",
    "# # Thu thập dữ liệu từ các collection\n",
    "# for review_collection in review_collections:\n",
    "#     documents = review_collection.find({'review/score': {'$exists': True}})\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review scores', position=0)\n",
    "    \n",
    "#     for document in documents:\n",
    "#         if 'Title' in document:\n",
    "#             title = document['Title']\n",
    "#             title_data[title].append(document['review/score'])\n",
    "#             progress_bar.update(1)\n",
    "\n",
    "#     progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(total=len(title_data), desc='Calculating review scores', position=0)\n",
    "\n",
    "# new_book_scores_list = list()\n",
    "\n",
    "# for title, scores_list in title_data.items():\n",
    "#     count_reviews = len(scores_list)\n",
    "#     if scores_list:\n",
    "#         mean_score = statistics.mean(scores_list)\n",
    "#         median_score = statistics.median(scores_list)\n",
    "#         mode_score = statistics.mode(scores_list)\n",
    "#     else:\n",
    "#         mean_score, median_score, mode_score = 0, 0, 0\n",
    "        \n",
    "#     book_score_info = ({'Title': title}, {'$set': {'review/mean_score': mean_score, \n",
    "#                                                        'review/median_score': median_score,\n",
    "#                                                        'review/mode_score': mode_score,\n",
    "#                                                        'review/count': count_reviews}})\n",
    "#     new_book_scores_list.append((book_score_info))\n",
    "\n",
    "#     progress_bar.update(1)\n",
    "\n",
    "# progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for book in new_book_scores_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             book[0],\n",
    "#             book[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews\n",
    "\n",
    "- Noises: Đã thực hiện ở trên\n",
    "- Missing values: Fill theo 'Id' nếu có 'Id' nào chứa Title đó. nếu không có để trống"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sử dụng defaultdict để tổ chức dữ liệu theo 'Title'\n",
    "# id_data = defaultdict(list)\n",
    "\n",
    "# progress_bar = tqdm(total=len(review_collections), desc='Preprocessing miss Review Titles', position=0)\n",
    "\n",
    "# # Thu thập dữ liệu từ các collection\n",
    "# for review_collection in review_collections:\n",
    "#     documents = review_collection.find({})\n",
    "    \n",
    "#     null_titles = review_collection.find({'Title': {'$exists': False}})\n",
    "    \n",
    "#     for document in null_titles:\n",
    "#         existed_titles_by_id_1 = review_collection_1.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_2 = review_collection_2.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_3 = review_collection_3.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_4 = review_collection_4.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_5 = review_collection_5.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_6 = review_collection_6.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_7 = review_collection_7.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "#         existed_titles_by_id_8 = review_collection_8.find({'Id': document['Id'], 'Title': {'$exists': True}})\n",
    "        \n",
    "#         for existed_review in existed_titles_by_id_1:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_2:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_3:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_4:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_5:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_6:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_7:\n",
    "#             print(existed_review)\n",
    "#         for existed_review in existed_titles_by_id_8:\n",
    "#             print(existed_review)\n",
    "    \n",
    "#     progress_bar.update(1)\n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     # update_requests = []\n",
    "\n",
    "#     # for published_date in published_date_list:\n",
    "#     #     update_requests.append(\n",
    "#     #         UpdateOne(\n",
    "#     #             published_date[0],\n",
    "#     #             published_date[1],\n",
    "#     #         )\n",
    "#     #     )\n",
    "        \n",
    "#     # print(update_requests[:5])\n",
    "\n",
    "#     # # Thực hiện bulk write\n",
    "#     # result = review_collection.bulk_write(update_requests)\n",
    "\n",
    "# progress_bar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết luận: Không có Title bị khuyết nào có thể được điền thông qua chỉ số Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường review/helpfulness\n",
    "- Chuyển từ dạng a/b thành dạng float\n",
    "- Thay missing values = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, review_collection in enumerate(review_collections):\n",
    "    \n",
    "#     print(f'--- Preprocessing title of review collection {index + 1} ---')\n",
    "#     helpfulness_list = []\n",
    "    \n",
    "#     progress_bar = tqdm(total=review_collection.count_documents({}), desc='Preprocessing review collection\\'s Helpfulness', position=0)\n",
    "    \n",
    "#     for index_2, review in enumerate(review_collection.find()):\n",
    "#         if 'review/helpfulness' in review:\n",
    "#             helpfulness_string = review['review/helpfulness']\n",
    "#             helpfulness_string = helpfulness_string.split('/')\n",
    "#             numerator = int(helpfulness_string[0])\n",
    "#             denominator = int(helpfulness_string[1])\n",
    "            \n",
    "#             # Nếu mẫu bằng 0\n",
    "#             if denominator == 0.0 or numerator == 0.0:\n",
    "#                 helpfulness_value = 0.0\n",
    "#             else:\n",
    "#                 helpfulness_value = numerator / denominator\n",
    "#                 if helpfulness_value > 1.0: # Xử lý các outlier lớn hơn 1\n",
    "#                     helpfulness_value = 1.0\n",
    "#         else:\n",
    "#             helpfulness_value = 0.0\n",
    "\n",
    "#         helpfulness_list.append(({'_id': review['_id']}, {'$set': {'review/helpfulness': helpfulness_value}}))\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()\n",
    "    \n",
    "#     # Tạo danh sách yêu cầu cập nhật\n",
    "#     update_requests = []\n",
    "    \n",
    "#     for helpfulness in helpfulness_list:\n",
    "#         update_requests.append(\n",
    "#             UpdateOne(\n",
    "#                 helpfulness[0],\n",
    "#                 helpfulness[1],\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     print(update_requests[:5])\n",
    "\n",
    "#     # Thực hiện bulk write\n",
    "#     result = review_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trường Authors và Categories\n",
    "- Chuyển về dạng list các tác giả và các danh mục\n",
    "- Chuyển về format chữ thường, loại bỏ kí tự đặc biệt như trong Authors và Categories collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def string_to_list(data):\n",
    "#     '''\n",
    "#     This function gets a list (authors, categories) from a string type\n",
    "    \n",
    "#     Args:\n",
    "#     data -- the input data needed to be converted\n",
    "    \n",
    "#     Returns:\n",
    "#     values -- the converted values (list type)\n",
    "#     '''\n",
    "#     # Process the string data to convert into list type\n",
    "#     values = data.strip(\"[]\").replace(\"'\", \"\")\n",
    "    \n",
    "#     # Remove the quotation marks\n",
    "#     values = values.replace('\"', '')\n",
    "    \n",
    "#     # Split the string to become a list\n",
    "#     values = values.split(\", \")\n",
    "    \n",
    "#     # Regex expression to remove invalid title\n",
    "#     pattern = re.compile(r'^[\\W\\d]+$')  # Remove strings containing only characters which are not \n",
    "\n",
    "#     # Remove invalid authors\n",
    "#     filtered_values = [value for value in values if not pattern.match(value)]\n",
    "    \n",
    "#     # Remove spaces at the beginning and ending of every element in the list\n",
    "#     values = list(value.strip() for value in filtered_values)\n",
    "    \n",
    "#     return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books = book_collection.find({'authors': {'$exists': True}})\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing...', position=0)\n",
    "\n",
    "# author_list = []\n",
    "\n",
    "# for book in books:\n",
    "#     new_authors = []\n",
    "#     authors = book['authors']\n",
    "    \n",
    "#     if (type(authors) == str):\n",
    "#         new_authors = string_to_list(authors)\n",
    "        \n",
    "#     elif (type(authors) == list):\n",
    "#         for author in authors:\n",
    "#             pattern = re.compile(r'^[\\W\\d]+$')  # Remove strings containing only characters which are not \n",
    "\n",
    "#             if author is not pattern.match(author):\n",
    "#                 continue\n",
    "            \n",
    "#             new_authors = authors\n",
    "    \n",
    "#     processed_authors = []\n",
    "#     for author in new_authors:\n",
    "#         author = author.lower()\n",
    "#         author = remove_special_characters(author)\n",
    "#         author = re.sub(r'\\s+', ' ', author)\n",
    "#         processed_authors.append(author)\n",
    "        \n",
    "#     author_list.append(({'_id': book['_id']}, {'$set': {'authors': processed_authors}}))\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "\n",
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for author in author_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             author[0],\n",
    "#             author[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books = book_collection.find({'categories': {'$exists': True}})\n",
    "# progress_bar = tqdm(total=total_book, desc='Preprocessing...', position=0)\n",
    "\n",
    "# category_list = []\n",
    "\n",
    "# for book in books:\n",
    "#     new_categories = []\n",
    "#     categories = book['categories']\n",
    "    \n",
    "#     if (type(categories) == str):\n",
    "#         new_categories = string_to_list(categories)\n",
    "        \n",
    "#     elif (type(categories) == list):\n",
    "#         for category in categories:\n",
    "#             pattern = re.compile(r'^[\\W\\d]+$')  # Remove strings containing only characters which are not \n",
    "\n",
    "#             if category is not pattern.match(category):\n",
    "#                 continue\n",
    "            \n",
    "#             new_categories = categories\n",
    "    \n",
    "#     processed_categories = []\n",
    "#     for category in new_categories:\n",
    "#         category = category.lower()\n",
    "#         category = remove_special_characters(category)\n",
    "#         category = re.sub(r'\\s+', ' ', category)\n",
    "#         processed_categories.append(category)\n",
    "        \n",
    "#     category_list.append(({'_id': book['_id']}, {'$set': {'categories': processed_categories}}))\n",
    "#     progress_bar.update(1)\n",
    "    \n",
    "# progress_bar.close()\n",
    "\n",
    "# # Tạo danh sách yêu cầu cập nhật\n",
    "# update_requests = []\n",
    "\n",
    "# for category in category_list:\n",
    "#     update_requests.append(\n",
    "#         UpdateOne(\n",
    "#             category[0],\n",
    "#             category[1],\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "# print(update_requests[:5])\n",
    "\n",
    "# # Thực hiện bulk write\n",
    "# # result = book_collection.bulk_write(update_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng Model:\n",
    "- SENTIMENT ANALYSIS đối với review/summary và review/text (RoBERTa-base model): biến chuỗi text thành dạng về một số (1: POS, 0: NEU, -1: NEG)\n",
    "- Tìm keyword của phần Description (BERT) (VD: Từ đoạn text \"Harry Potter this year fights Lord Voldemort and his Death Eaters\" -> [\"Harry Potter\", \"Lord Voldemort\", \"Death Eaters\"] -> \"Harry Potter Lord Voldemort Death Eaters\") -> Tính TF-IDF của phần Description giữa review input và toàn bộ review trong database -> Cho ra similarity với Description đầu vào\n",
    "- Tính TF-IDF các mục Authors, Title, Categories -> Cho ra similarity với Authors, Title, Categories đầu vào \n",
    "- Tính kNN (cosine similarity) giữa vector review vừa được thêm vào với tất cả review còn lại rồi trả về top 10 quyển có chỉ số sim cao nhất\n",
    "\n",
    "Cuối cùng là display nó lên cho người dùng xem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = []\n",
    "\n",
    "# for index, review_collection in enumerate(review_collections):\n",
    "#     print(f' --- Start converting review collection {index + 1} ---')\n",
    "#     df_review = pd.DataFrame(list(review_collection.find()))\n",
    "    \n",
    "#     dd_review = dd.from_pandas(df_review, npartitions=4)\n",
    "#     print(f' --- Done converting review collection {index + 1} ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews = df.concat(df_reviews)\n",
    "# df_book = book_collection.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_book_selected = df_book.loc[:, ['Title', 'authors', 'categories', 'description', 'review/count', 'review/mean_score', 'review/median_score', 'review/mode_score']]\n",
    "# df_reviews_selected = df_reviews.loc[:, ['Title', 'User_id', 'review/score', 'review/summary', 'review/text']]\n",
    "# merged_df = pd.merge(df_reviews_selected, df_book_selected, on='Title', how='inner')\n",
    "\n",
    "# merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv('D:\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file all.csv lên lại để đưa vào mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    # text = \"This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the place to find it -- there's only about 2 pages with text and everything else is photos.Bottom line: if you only want one book, the Six Foot One ... is probably a better choice, however, if you like Julie like I like Julie, you won't go wrong on this one either.\"\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "\n",
    "    highest_score = max(scores)\n",
    "    label = config.id2label[ranking[0]]\n",
    "\n",
    "    return highest_score, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffile = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")\n",
    "\n",
    "# ffile['sentiment'] = ''\n",
    "\n",
    "# ffile.to_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 512\n",
    "# CHUNK_SIZE = 100\n",
    "\n",
    "# use_cols = ['Title', 'User_id', 'review/text']\n",
    "\n",
    "# df_all = pd.read_csv(\"D:\\\\University\\Môn học\\Ứng dụng Big Data\\Project\\Datasets\\Amazon Books Reviews\\\\all.csv\",\n",
    "#                      chunksize=CHUNK_SIZE,\n",
    "#                      usecols=use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# count = int(3500000 / CHUNK_SIZE)\n",
    "\n",
    "# sentiment_list = []\n",
    "\n",
    "# progress_bar = tqdm(total=count, desc=f'Processing Sentiment Analysis...', position=0)\n",
    "\n",
    "# with open('sentiment.json', 'w') as json_file:\n",
    "    \n",
    "#     for chunk in df_all:\n",
    "        \n",
    "#         for index, row in chunk.iterrows():\n",
    "            \n",
    "#             review_text = html.unescape(row['review/text'])\n",
    "            \n",
    "#             title = row['Title']\n",
    "#             if title == '' or title == None:\n",
    "#                 continue\n",
    "            \n",
    "#             user_id = str(row['User_id'])\n",
    "#             if user_id == '' or user_id == None:\n",
    "#                 user_id = ''\n",
    "            \n",
    "#             if review_text == '' or review_text == None:\n",
    "#                 sentiment = 0\n",
    "#                 break\n",
    "\n",
    "#             sentiment_score, label = sentiment_analysis(review_text[:max_length])\n",
    "\n",
    "#             if label == 'positive':\n",
    "#                 sentiment = 1\n",
    "#             elif label == 'neutral':\n",
    "#                 sentiment = 0\n",
    "#             elif label == 'negative':\n",
    "#                 sentiment = -1\n",
    "                \n",
    "#             sentiment_json = {'Title': title, 'User_id': user_id, 'sentiment': sentiment}\n",
    "#             json.dump(sentiment_json, json_file)\n",
    "#             json_file.write(',\\n')\n",
    "#         progress_bar.update(1)\n",
    "        \n",
    "#     progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "count = 0\n",
    "sentiment_list = []\n",
    "\n",
    "for review_collection in review_collections:\n",
    "\n",
    "    reviews = review_collection.find()\n",
    "    existed_num = review_collection.count_documents({'sentiment': {'$exists': True}})\n",
    "    progress_bar = tqdm(total= (total_review - existed_num), desc=f'Processing Sentiment Analysis...', position=0)\n",
    "    \n",
    "    for review in reviews.skip(existed_num):\n",
    "        \n",
    "        review_text = html.unescape(str(review['review/text']))\n",
    "        \n",
    "        if review_text == '' or review_text is None:\n",
    "            sentiment = 0\n",
    "        else:\n",
    "            sentiment_score, label = sentiment_analysis(review_text[:max_length])\n",
    "            if label == 'positive':\n",
    "                sentiment = 1\n",
    "            elif label == 'neutral':\n",
    "                sentiment = 0\n",
    "            elif label == 'negative':\n",
    "                sentiment = -1\n",
    "                \n",
    "        sentiment_list.append(({'_id': review['_id']}, {'$set': {'sentiment': sentiment}}))\n",
    "        \n",
    "        if count % 1000 == 0:\n",
    "            update_requests = []\n",
    "\n",
    "            for sentiment in sentiment_list:\n",
    "                update_requests.append(\n",
    "                    UpdateOne(\n",
    "                        sentiment[0],\n",
    "                        sentiment[1]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Thực hiện bulk write\n",
    "            result = review_collection_1.bulk_write(update_requests)\n",
    "            sentiment_list = []\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
